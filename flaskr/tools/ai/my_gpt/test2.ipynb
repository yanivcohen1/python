{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0e9ae8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n",
      "CropBox missing from /Page, defaulting to MediaBox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 11.0327, val loss 10.9140, delta 10.9140 --------------------------\n",
      "RL iter 0: R=0.0000 | Gen=' tray regulations cons | A=She grows larger'\n",
      "RL iter 50: R=1.0000 | Gen=' mocking\n",
      "lived\n",
      " created | A=The Knave of Hearts'\n",
      "step 100: train loss 5.9415, val loss 7.4600, delta -3.4539 --------------------------\n",
      "RL iter 100: R=0.0000 | Gen='? be | A=Lewis Carroll'\n",
      "RL iter 150: R=0.4159 | Gen=' for so his all in that andly melancholyd, ’ Metal | A=It is organized to dry the party after they all fall into a pool of tears'\n",
      "step 200: train loss 5.4605, val loss 7.3951, delta -0.0650 --------------------------\n",
      "RL iter 200: R=0.1918 | Gen=' work Who under _!, | A=Down the Rabbit-Hole'\n",
      "RL iter 250: R=0.4877 | Gen=' her,� hadout Duchess: _Ipled. for”\n",
      " | A=The text includes the full Project Gutenberg license and terms of use for the ebook'\n",
      "step 300: train loss 5.2032, val loss 7.3321, delta -0.0630 --------------------------\n",
      "RL iter 300: R=1.0000 | Gen=' been of” heand | A=\"Off with his head!\"'\n",
      "RL iter 350: R=0.5121 | Gen='“ tongue: entitywindow they of and at it!� | A=Absurd, whimsical, chaotic, and filled with nonsensical logic'\n",
      "step 400: train loss 5.0100, val loss 7.2958, delta -0.0363 --------------------------\n",
      "RL iter 400: R=0.8310 | Gen='’s—on it. sheq Facebook; And ” said the King | A=That she is changing and sometimes does not feel like the person she was at the start'\n",
      "RL iter 450: R=1.0000 | Gen='“I the heard� | A=\"Off with his head!\"'\n",
      "step 500: train loss 4.9328, val loss 7.3459, delta 0.0501 --------------------------\n",
      "RL iter 500: R=0.0000 | Gen='“ | A=Lewis Carroll'\n",
      "RL iter 550: R=0.1636 | Gen=' and call up a | A=The Cheshire Cat'\n",
      "step 600: train loss 4.7215, val loss 7.3195, delta -0.0264 --------------------------\n",
      "RL iter 600: R=0.5345 | Gen='\n",
      "G\n",
      "“You rid went on | A=He also acts as a herald during the trial'\n",
      "RL iter 650: R=0.4706 | Gen=' “Why say“ | A=Down the Rabbit-Hole'\n",
      "step 700: train loss 4.6129, val loss 7.2556, delta -0.0639 --------------------------\n",
      "RL iter 700: R=0.5968 | Gen='\n",
      "“What to Duchess more oh, in mockedinside used it | A=Live flamingos used as mallets, hedgehogs as balls, and soldiers doubling as arches'\n",
      "RL iter 750: R=0.4783 | Gen='\n",
      "posted! | A=A small cake'\n",
      "step 800: train loss 4.5071, val loss 7.2560, delta 0.0005 --------------------------\n",
      "RL iter 800: R=0.5949 | Gen='\n",
      "He\n",
      "question | A=It makes her shrink'\n",
      "RL iter 850: R=0.6072 | Gen='\n",
      "plate face | A=A small cake'\n",
      "step 900: train loss 4.3863, val loss 7.3018, delta 0.0458 --------------------------\n",
      "RL iter 900: R=0.5763 | Gen=' “theMem—Once. I’s advisable.\n",
      "without at out | A=Alice wakes up on the riverbank, realizing that her adventures in Wonderland were a dream'\n",
      "RL iter 950: R=0.5773 | Gen=' “ seeain, � | A=\"Off with his head!\"'\n",
      "step 1000: train loss 4.2958, val loss 7.2905, delta -0.0114 --------------------------\n",
      "RL iter 1000: R=0.6836 | Gen=' and�\n",
      "think the seahole, tax answer,\n",
      "time\n",
      "the,5 | A=Alice wakes up on the riverbank, realizing that her adventures in Wonderland were a dream'\n",
      "RL iter 1050: R=0.4110 | Gen=' death 1. Project Gutenberg™4,\n",
      "Alice of www comply | A=It is chaotic, with everyone talking, running, and arguing without a clear order'\n",
      "step 1100: train loss 4.2497, val loss 7.4296, delta 0.1392 --------------------------\n",
      "RL iter 1100: R=0.5172 | Gen='\n",
      " 144 thousandched was isn“ transcription, I don”  | A=It is organized to dry the party after they all fall into a pool of tears'\n",
      "RL iter 1150: R=0.7643 | Gen=' “some | A=A small cake'\n",
      "step 1200: train loss 4.1513, val loss 7.4074, delta -0.0222 --------------------------\n",
      "RL iter 1200: R=0.4706 | Gen='S of the paper was exactly as the chimney took the | A=\"Why is a raven like a writing-desk?\"'\n",
      "RL iter 1250: R=0.6677 | Gen=' “as for shegan of the directlyers),” “If go | A=Alice wakes up on the riverbank, realizing that her adventures in Wonderland were a dream'\n",
      "step 1300: train loss 4.1085, val loss 7.3797, delta -0.0277 --------------------------\n",
      "RL iter 1300: R=0.2052 | Gen=' “if: | A=It makes her shrink'\n",
      "RL iter 1350: R=0.5362 | Gen=' “Nobody it’s quite\n",
      "que, you don wonder was exactl | A=On a mix of nonsensical evidence and arbitrary rules, such as Rule Forty-two'\n",
      "step 1400: train loss 4.0255, val loss 7.4696, delta 0.0900 --------------------------\n",
      "RL iter 1400: R=0.4717 | Gen=' very nearly ME crossed?”\n",
      "and narrow such the Fish | A=The text includes the full Project Gutenberg license and terms of use for the ebook'\n",
      "RL iter 1450: R=0.2603 | Gen=' and�You hasnnoon | A=Down the Rabbit-Hole'\n",
      "step 1500: train loss 3.9322, val loss 7.5331, delta 0.0635 --------------------------\n",
      "RL iter 1500: R=0.4863 | Gen=' a long\n",
      "of the Project\n",
      "little pleasant\n",
      "tell it of  | A=It is chaotic, with everyone talking, running, and arguing without a clear order'\n",
      "RL iter 1550: R=0.4904 | Gen='JECT G song, Don� | A=\"Off with his head!\"'\n",
      "step 1600: train loss 3.8755, val loss 7.4782, delta -0.0549 --------------------------\n",
      "RL iter 1600: R=0.6392 | Gen=' knocked. “howapp little sisters!’t believe | A=She falls down a deep rabbit-hole into a strange world.'\n",
      "RL iter 1650: R=0.2916 | Gen=' donations to do, | A=The Cheshire Cat'\n",
      "step 1700: train loss 3.7887, val loss 7.4985, delta 0.0204 --------------------------\n",
      "RL iter 1700: R=0.0000 | Gen=' “ | A=Lewis Carroll'\n",
      "RL iter 1750: R=0.6292 | Gen=' a low possessed: “But it_ solemn_ be dragWhen fir | A=Live flamingos used as mallets, hedgehogs as balls, and soldiers doubling as arches'\n",
      "step 1800: train loss 3.7998, val loss 7.6307, delta 0.1322 --------------------------\n",
      "RL iter 1800: R=0.6213 | Gen='tain-Party or\n",
      "door the workand holding, you know.  | A=That she is changing and sometimes does not feel like the person she was at the start'\n",
      "RL iter 1850: R=0.4355 | Gen=' but ever | A=Lewis Carroll'\n",
      "step 1900: train loss 3.7274, val loss 7.6032, delta -0.0275 --------------------------\n",
      "RL iter 1900: R=0.4928 | Gen=' the Project Gutenberg™ electronic nextfully in\n",
      "th | A=On a mix of nonsensical evidence and arbitrary rules, such as Rule Forty-two'\n",
      "RL iter 1950: R=0.5401 | Gen=', and taking you manage, There was Zealand\n",
      "should  | A=A small bottle with a paper label that instructs her to drink it'\n",
      "step 2000: train loss 3.6993, val loss 7.7165, delta 0.1133 --------------------------\n",
      "RL iter 2000: R=0.6617 | Gen=' this agreement. However, and the P inches speech2 | A=It is organized to dry the party after they all fall into a pool of tears'\n",
      "RL iter 2050: R=0.7038 | Gen=' hisager forgottenster-fishply�are and the officer | A=The Project Gutenberg eBook of Alice’s Adventures in Wonderland'\n",
      "step 2100: train loss 3.6932, val loss 7.8488, delta 0.1323 --------------------------\n",
      "RL iter 2100: R=0.2142 | Gen=' the First. | A=A small cake'\n",
      "RL iter 2150: R=0.6872 | Gen=' the time, but this agreement in a players “How an | A=She sees a White Rabbit speaking and notices it has a waistcoat-pocket and a watch.'\n",
      "step 2200: train loss 3.6563, val loss 7.9127, delta 0.0639 --------------------------\n",
      "RL iter 2200: R=0.5949 | Gen=' the work did and hisBooks | A=\"Off with his head!\"'\n",
      "RL iter 2250: R=0.2052 | Gen=' the pieces— | A=A small cake'\n",
      "step 2300: train loss 3.5424, val loss 7.9082, delta -0.0045 --------------------------\n",
      "RL iter 2300: R=0.6022 | Gen=' the locks, so look | A=The Knave of Hearts'\n",
      "RL iter 2350: R=0.8120 | Gen=' the school to keep | A=It makes her shrink'\n",
      "step 2400: train loss 3.4906, val loss 8.0675, delta 0.1592 --------------------------\n",
      "RL iter 2400: R=0.8517 | Gen=' the Gryphon | A=It makes her shrink'\n",
      "RL iter 2450: R=0.6416 | Gen=' Alice. “we she was in a means.\n",
      "” it in the White  | A=She sees a White Rabbit speaking and notices it has a waistcoat-pocket and a watch.'\n",
      "step 2500: train loss 3.5254, val loss 7.8621, delta -0.2054 --------------------------\n",
      "RL iter 2500: R=0.4892 | Gen='’s all played, very civil (she tried; and | A=Absurd, whimsical, chaotic, and filled with nonsensical logic'\n",
      "RL iter 2550: R=0.5303 | Gen=' “Right them; more energetic way when the\n",
      "silon to | A=A small bottle with a paper label that instructs her to drink it'\n",
      "step 2600: train loss 3.4806, val loss 7.8175, delta -0.0446 --------------------------\n",
      "RL iter 2600: R=0.4886 | Gen=' “How and it set to another shore of great might\n",
      "’ | A=It is chaotic, with everyone talking, running, and arguing without a clear order'\n",
      "RL iter 2650: R=0.6333 | Gen=' “it’time vanished, Alice,\n",
      " ANYends, let the King | A=That she is changing and sometimes does not feel like the person she was at the start'\n",
      "step 2700: train loss 3.4620, val loss 7.7543, delta -0.0632 --------------------------\n",
      "RL iter 2700: R=0.1485 | Gen=' “But it might | A=The Knave of Hearts'\n",
      "RL iter 2750: R=0.3741 | Gen='’taste the Hatter did her sister. The | A=\"Why is a raven like a writing-desk?\"'\n",
      "step 2800: train loss 3.3924, val loss 7.8143, delta 0.0600 --------------------------\n",
      "RL iter 2800: R=0.5852 | Gen='”\n",
      " happen might, look yourself Alice replied _I | A=\"Why is a raven like a writing-desk?\"'\n",
      "RL iter 2850: R=0.6173 | Gen=' it might, or was just begun just begun the\n",
      "to | A=The Project Gutenberg eBook of Alice’s Adventures in Wonderland'\n",
      "step 2900: train loss 3.4038, val loss 7.9211, delta 0.1068 --------------------------\n",
      "RL iter 2900: R=0.6769 | Gen=' it might, it is to its mind noticed their\n",
      " | A=A set of verses that seem to have no clear meaning'\n",
      "RL iter 2950: R=0.6928 | Gen=' perhaps a\n",
      " ME say the Gryphon lazhow-and\n",
      "(tradem | A=That she is changing and sometimes does not feel like the person she was at the start'\n",
      "step 3000: train loss 3.4004, val loss 7.9686, delta 0.0475 --------------------------\n",
      "RL iter 3000: R=0.4646 | Gen=' she tried this agreement.\n",
      "” Alice.\n",
      "The jury | A=She falls down a deep rabbit-hole into a strange world.'\n",
      "RL iter 3050: R=0.6259 | Gen=' then nonsense must two, “unless it might better b | A=Alice wakes up on the riverbank, realizing that her adventures in Wonderland were a dream'\n",
      "step 3100: train loss 3.3781, val loss 7.9075, delta -0.0611 --------------------------\n",
      "RL iter 3100: R=0.7389 | Gen=' then nonsense must two comply.\n",
      "” Alice, who | A=The Project Gutenberg eBook of Alice’s Adventures in Wonderland'\n",
      "RL iter 3150: R=0.5209 | Gen=' then nonsense | A=Lewis Carroll'\n",
      "step 3200: train loss 3.3102, val loss 7.9980, delta 0.0905 --------------------------\n",
      "RL iter 3200: R=0.5989 | Gen=' then nonsense must two-way\n",
      "Alice’t grown up off q | A=It is chaotic, with everyone talking, running, and arguing without a clear order'\n",
      "RL iter 3250: R=0.3901 | Gen=' then nonsense must two-hand, then nonsense must t | A=The Project Gutenberg eBook of Alice’s Adventures in Wonderland'\n",
      "step 3300: train loss 3.2603, val loss 7.9829, delta -0.0152 --------------------------\n",
      "RL iter 3300: R=0.5813 | Gen=' then nonsense must two-way Project Gutenberg Lite | A=\"Why is a raven like a writing-desk?\"'\n",
      "RL iter 3350: R=0.5030 | Gen=' Project Gutenberg Literary\n",
      " | A=A little golden key'\n",
      "step 3400: train loss 3.2233, val loss 8.0770, delta 0.0941 --------------------------\n",
      "RL iter 3400: R=0.2705 | Gen=' croquet copyright law.\n",
      "Alice found her arm agains | A=She sees a White Rabbit speaking and notices it has a waistcoat-pocket and a watch.'\n",
      "RL iter 3450: R=0.5852 | Gen=' it might, hold\n",
      "” called some fallen into another  | A=He presides over it and even writes down rules and evidence, despite the absurdity of the proceedings'\n",
      "step 3500: train loss 3.2517, val loss 8.1975, delta 0.1205 --------------------------\n",
      "RL iter 3500: R=0.6061 | Gen=' never\n",
      "us to whom I wonder she got burnt, donation | A=It changes unpredictably—she shrinks after drinking from a bottle and grows after eating a cake'\n",
      "RL iter 3550: R=0.1399 | Gen=' may be tramAs’ | A=\"Off with his head!\"'\n",
      "step 3600: train loss 3.1934, val loss 8.1342, delta -0.0632 --------------------------\n",
      "RL iter 3600: R=0.7268 | Gen=' perhaps now | A=Lewis Carroll'\n",
      "RL iter 3650: R=0.4737 | Gen=' four times!”\n",
      "“I quite forgot do you wish you have | A=Live flamingos used as mallets, hedgehogs as balls, and soldiers doubling as arches'\n",
      "step 3700: train loss 3.2055, val loss 8.1318, delta -0.0025 --------------------------\n",
      "RL iter 3700: R=0.4118 | Gen=' four times there are old conger, but the work, th | A=Alice wakes up on the riverbank, realizing that her adventures in Wonderland were a dream'\n",
      "RL iter 3750: R=0.5059 | Gen=' downwards, but this same word processing or flapp | A=She sees a White Rabbit speaking and notices it has a waistcoat-pocket and a watch.'\n",
      "step 3800: train loss 3.1731, val loss 8.1148, delta -0.0169 --------------------------\n",
      "RL iter 3800: R=0.6882 | Gen=' _ever_e hours!” yelled but it (or_ twist us: for  | A=She sees a White Rabbit speaking and notices it has a waistcoat-pocket and a watch.'\n",
      "RL iter 3850: R=0.2470 | Gen='!’t | A=The Cheshire Cat'\n",
      "step 3900: train loss 3.1734, val loss 8.2066, delta 0.0918 --------------------------\n",
      "RL iter 3900: R=0.4945 | Gen=' never!’ll stay Dinah!” the Queen\n",
      "“ | A=On a mix of nonsensical evidence and arbitrary rules, such as Rule Forty-two'\n",
      "RL iter 3950: R=0.6677 | Gen=' Queens, in the | A=A little golden key'\n",
      "step 4000: train loss 3.1169, val loss 8.1365, delta -0.0701 --------------------------\n",
      "RL iter 4000: R=0.3647 | Gen=' donations must www. | A=A little golden key'\n",
      "RL iter 4050: R=0.7556 | Gen=' four times!” the Rabbit. “we goes his tried to se | A=Live flamingos used as mallets, hedgehogs as balls, and soldiers doubling as arches'\n",
      "step 4100: train loss 3.1582, val loss 8.1561, delta 0.0195 --------------------------\n",
      "RL iter 4100: R=0.5000 | Gen=' four times! Take within 90 days Literary their mi | A=The text includes the full Project Gutenberg license and terms of use for the ebook'\n",
      "RL iter 4150: R=0.4283 | Gen=' four times! How puzzling-box: four times. The gam | A=It changes unpredictably—she shrinks after drinking from a bottle and grows after eating a cake'\n",
      "step 4200: train loss 3.1107, val loss 8.1876, delta 0.0315 --------------------------\n",
      "RL iter 4200: R=0.5871 | Gen=' states of the end of it except his Normarseers: n | A=It changes unpredictably—she shrinks after drinking from a bottle and grows after eating a cake'\n",
      "RL iter 4250: R=0.3128 | Gen=' nowhere at hisBooks ex head: the little thing alo | A=Absurd, whimsical, chaotic, and filled with nonsensical logic'\n",
      "step 4300: train loss 3.1117, val loss 8.2270, delta 0.0394 --------------------------\n",
      "RL iter 4300: R=0.6226 | Gen=' perhaps in the cook confusing away, and found dow | A=A set of verses that seem to have no clear meaning'\n",
      "RL iter 4350: R=0.7587 | Gen=' perhaps in the Gryphon, while finishing | A=He also acts as a herald during the trial'\n",
      "step 4400: train loss 3.0827, val loss 8.2794, delta 0.0524 --------------------------\n",
      "RL iter 4400: R=0.5550 | Gen=' perhaps in the Gryphon! How puzzling-,\n",
      "being drow | A=He presides over it and even writes down rules and evidence, despite the absurdity of the proceedings'\n",
      "RL iter 4450: R=0.5717 | Gen=' perhaps in the Caterpillar. | A=\"Off with his head!\"'\n",
      "step 4500: train loss 3.0331, val loss 8.3069, delta 0.0275 --------------------------\n",
      "RL iter 4500: R=0.5336 | Gen=' perhaps in the she did upon 1.\n",
      "Though down no lab | A=Alice wakes up on the riverbank, realizing that her adventures in Wonderland were a dream'\n",
      "RL iter 4550: R=0.8517 | Gen=' perhaps in the Gryphon, alas! | A=He also acts as a herald during the trial'\n",
      "step 4600: train loss 3.0256, val loss 8.3320, delta 0.0251 --------------------------\n",
      "RL iter 4600: R=0.5246 | Gen=' perhaps in the laughter. | A=The Knave of Hearts'\n",
      "RL iter 4650: R=0.4669 | Gen=' perhaps in the little more: perhaps in sight! Nob | A=Absurd, whimsical, chaotic, and filled with nonsensical logic'\n",
      "step 4700: train loss 2.9846, val loss 8.3338, delta 0.0018 --------------------------\n",
      "RL iter 4700: R=0.4139 | Gen=' perhaps in sight in sight! The Foundation” then!\n",
      " | A=On a mix of nonsensical evidence and arbitrary rules, such as Rule Forty-two'\n",
      "RL iter 4750: R=0.6030 | Gen=' perhaps in sight in the little—I let perhaps in _ | A=On a mix of nonsensical evidence and arbitrary rules, such as Rule Forty-two'\n",
      "step 4800: train loss 3.0167, val loss 8.5030, delta 0.1692 --------------------------\n",
      "RL iter 4800: R=0.6830 | Gen=' perhaps in in!\n",
      "” sighed�Suppose you shouldn’ll be | A=Live flamingos used as mallets, hedgehogs as balls, and soldiers doubling as arches'\n",
      "RL iter 4850: R=0.6751 | Gen=' perhaps in in, then!\n",
      "’t want in you see please be | A=It changes unpredictably—she shrinks after drinking from a bottle and grows after eating a cake'\n",
      "step 4900: train loss 3.0447, val loss 8.3517, delta -0.1514 --------------------------\n",
      "RL iter 4900: R=0.5368 | Gen=' perhaps you want you see?” added for the Mouse wa | A=Live flamingos used as mallets, hedgehogs as balls, and soldiers doubling as arches'\n",
      "RL iter 4950: R=0.4169 | Gen=' perhaps you want you want in Alice, you be very c | A=That she is changing and sometimes does not feel like the person she was at the start'\n",
      "step 5000: train loss 2.9924, val loss 8.3748, delta 0.0231 --------------------------\n",
      "RL iter 5000: R=0.6192 | Gen=' perhaps you want as she thought, you want in you  | A=A small bottle with a paper label that instructs her to drink it'\n",
      "RL iter 5050: R=0.7268 | Gen=' perhaps you answer as she thought, you begun Alic | A=The text includes the full Project Gutenberg license and terms of use for the ebook'\n",
      "step 5100: train loss 2.9729, val loss 8.4098, delta 0.0350 --------------------------\n",
      "RL iter 5100: R=0.3315 | Gen=' you wish you | A=She grows larger'\n",
      "RL iter 5150: R=0.1323 | Gen=' you wish you want | A=A little golden key'\n",
      "step 5200: train loss 2.8990, val loss 8.4079, delta -0.0019 --------------------------\n",
      "RL iter 5200: R=0.6544 | Gen=' you wish you want you want as you want to abide y | A=She falls down a deep rabbit-hole into a strange world.'\n",
      "RL iter 5250: R=0.2782 | Gen=' you want as you want as you want as you want as | A=\"Why is a raven like a writing-desk?\"'\n",
      "step 5300: train loss 2.8891, val loss 8.4789, delta 0.0710 --------------------------\n",
      "RL iter 5300: R=0.5030 | Gen=' you want as | A=She grows larger'\n",
      "RL iter 5350: R=0.4355 | Gen=' you want as you want as you like him cried the Ca | A=It is chaotic, with everyone talking, running, and arguing without a clear order'\n",
      "step 5400: train loss 2.8406, val loss 8.3945, delta -0.0844 --------------------------\n",
      "RL iter 5400: R=0.5058 | Gen=' you like some book!\n",
      "” lonely stateunderhow DISTRI | A=He presides over it and even writes down rules and evidence, despite the absurdity of the proceedings'\n",
      "RL iter 5450: R=0.5191 | Gen=' you see, you like a more!\n",
      " reach took | A=A set of verses that seem to have no clear meaning'\n",
      "step 5500: train loss 2.8840, val loss 8.4343, delta 0.0398 --------------------------\n",
      "RL iter 5500: R=0.4904 | Gen=' you see, you see, | A=Down the Rabbit-Hole'\n",
      "RL iter 5550: R=0.6091 | Gen=' there may not located down the whole court; and t | A=\"Why is a raven like a writing-desk?\"'\n",
      "step 5600: train loss 2.8646, val loss 8.4200, delta -0.0143 --------------------------\n",
      "RL iter 5600: R=0.4053 | Gen=' “I think.’d have liked teaching what\n",
      "� | A=A small bottle with a paper label that instructs her to drink it'\n",
      "RL iter 5650: R=0.3806 | Gen=' “I think.’d have liked teaching what would be | A=A small bottle with a paper label that instructs her to drink it'\n",
      "step 5700: train loss 2.8199, val loss 8.4379, delta 0.0179 --------------------------\n",
      "RL iter 5700: R=0.5240 | Gen=' there had been rather doubtful once. but this,\n",
      ":\n",
      " | A=That she is changing and sometimes does not feel like the person she was at the start'\n",
      "RL iter 5750: R=0.5371 | Gen='\n",
      "“Do you learn? I don’t them,” | A=It is chaotic, with everyone talking, running, and arguing without a clear order'\n",
      "step 5800: train loss 2.8692, val loss 8.4837, delta 0.0458 --------------------------\n",
      "RL iter 5800: R=0.9029 | Gen='\n",
      "only now that he could | A=Down the Rabbit-Hole'\n",
      "RL iter 5850: R=0.5949 | Gen=' “so, only does, “and there’t be to grow larger ag | A=She sees a White Rabbit speaking and notices it has a waistcoat-pocket and a watch.'\n",
      "step 5900: train loss 2.8914, val loss 8.4692, delta -0.0145 --------------------------\n",
      "RL iter 5900: R=0.8314 | Gen='\n",
      "whole the Queen, | A=Down the Rabbit-Hole'\n",
      "RL iter 5950: R=1.0000 | Gen=' the centre of the | A=The Cheshire Cat'\n",
      "step 6000: train loss 2.8290, val loss 8.6633, delta 0.1940 --------------------------\n",
      "RL iter 6000: R=0.6135 | Gen=' the Owl. � | A=It makes her shrink'\n",
      "RL iter 6050: R=0.5773 | Gen=' this,) you insolgh I’ve _very_ don_ stuff, come u | A=It changes unpredictably—she shrinks after drinking from a bottle and grows after eating a cake'\n",
      "step 6100: train loss 2.8391, val loss 8.6661, delta 0.0028 --------------------------\n",
      "RL iter 6100: R=0.6135 | Gen=' thismediate access to this, with these words you\n",
      " | A=It is organized to dry the party after they all fall into a pool of tears'\n",
      "RL iter 6150: R=0.3839 | Gen=' theormouse, and (801, CONSEQUENTIAL | A=Absurd, whimsical, chaotic, and filled with nonsensical logic'\n",
      "step 6200: train loss 2.8179, val loss 8.5293, delta -0.1368 --------------------------\n",
      "RL iter 6200: R=0.6279 | Gen=' theormouse down no notice and a knife,\n",
      "arch | A=The Project Gutenberg eBook of Alice’s Adventures in Wonderland'\n",
      "RL iter 6250: R=0.7886 | Gen='\n",
      "door to be like the look up curled messages with  | A=That she is changing and sometimes does not feel like the person she was at the start'\n",
      "step 6300: train loss 2.9035, val loss 8.4264, delta -0.1030 --------------------------\n",
      "RL iter 6300: R=0.4405 | Gen=' the use\n",
      "door to do him\n",
      "ers instantly again was | A=\"Why is a raven like a writing-desk?\"'\n",
      "RL iter 6350: R=0.7084 | Gen='\n",
      "door to do him into the month terms of the make m | A=She falls down a deep rabbit-hole into a strange world.'\n",
      "step 6400: train loss 2.8611, val loss 8.4715, delta 0.0451 --------------------------\n",
      "RL iter 6400: R=0.3943 | Gen='\n",
      "shook her became | A=The Knave of Hearts'\n",
      "RL iter 6450: R=0.2751 | Gen='\n",
      "have | A=Lewis Carroll'\n",
      "step 6500: train loss 2.7573, val loss 8.5064, delta 0.0349 --------------------------\n",
      "RL iter 6500: R=0.3997 | Gen='\n",
      "door to | A=She grows larger'\n",
      "RL iter 6550: R=0.7587 | Gen='\n",
      "door to the make me\n",
      "you know.� | A=A set of verses that seem to have no clear meaning'\n",
      "step 6600: train loss 2.7699, val loss 8.5607, delta 0.0543 --------------------------\n",
      "RL iter 6600: R=0.2675 | Gen='\n",
      "executed, CONSE | A=Down the Rabbit-Hole'\n",
      "RL iter 6650: R=0.5209 | Gen=' this?\n",
      "door | A=The Cheshire Cat'\n",
      "step 6700: train loss 2.6953, val loss 8.5294, delta -0.0312 --------------------------\n",
      "RL iter 6700: R=0.4030 | Gen=' “his opinion.” I\n",
      " | A=He also acts as a herald during the trial'\n",
      "RL iter 6750: R=0.7268 | Gen=' Bill, | A=Lewis Carroll'\n",
      "step 6800: train loss 2.6892, val loss 8.5425, delta 0.0130 --------------------------\n",
      "RL iter 6800: R=0.6552 | Gen=' beautiful Soup\n",
      "took messages more happened all ov | A=He presides over it and even writes down rules and evidence, despite the absurdity of the proceedings'\n",
      "RL iter 6850: R=0.6405 | Gen=' beautiful Soup\n",
      "Rabbit interrupted curled messages | A=That she is changing and sometimes does not feel like the person she was at the start'\n",
      "step 6900: train loss 2.7613, val loss 8.5631, delta 0.0206 --------------------------\n",
      "RL iter 6900: R=0.5933 | Gen=' beautiful Soup that will sing it down,.\n",
      "”\n",
      "It was  | A=He presides over it and even writes down rules and evidence, despite the absurdity of the proceedings'\n",
      "RL iter 6950: R=0.3917 | Gen=' it out! Don”\n",
      "It was his cheeks. | A=The Project Gutenberg eBook of Alice’s Adventures in Wonderland'\n",
      "step 7000: train loss 2.7129, val loss 8.5686, delta 0.0055 --------------------------\n",
      "RL iter 7000: R=0.6747 | Gen=' his cheeks.\n",
      "”\n",
      "It was her knowledge Literary more  | A=It is chaotic, with everyone talking, running, and arguing without a clear order'\n",
      "RL iter 7050: R=0.6049 | Gen=' beautiful Soup!”\n",
      "“Of course you | A=A set of verses that seem to have no clear meaning'\n",
      "step 7100: train loss 2.6990, val loss 8.7528, delta 0.1843 --------------------------\n",
      "RL iter 7100: R=0.7529 | Gen=' your stateter in a tunnel directions will sing fr | A=\"Why is a raven like a writing-desk?\"'\n",
      "RL iter 7150: R=0.3103 | Gen=' “so | A=A small cake'\n",
      "step 7200: train loss 2.6726, val loss 8.6065, delta -0.1463 --------------------------\n",
      "RL iter 7200: R=0.4311 | Gen=' your stateter to this moment Alice.\n",
      "”\n",
      "her knowled | A=It is chaotic, with everyone talking, running, and arguing without a clear order'\n",
      "RL iter 7250: R=0.7561 | Gen=' your stateter in a serpent: your state of uglific | A=She falls down a deep rabbit-hole into a strange world.'\n",
      "step 7300: train loss 2.6827, val loss 8.8649, delta 0.2584 --------------------------\n",
      "RL iter 7300: R=0.5608 | Gen=' your state of them.\n",
      "” � | A=He also acts as a herald during the trial'\n",
      "RL iter 7350: R=0.8896 | Gen=' your state of them. | A=The Knave of Hearts'\n",
      "step 7400: train loss 2.6056, val loss 8.8288, delta -0.0361 --------------------------\n",
      "RL iter 7400: R=0.5831 | Gen=' perhaps he | A=Lewis Carroll'\n",
      "RL iter 7450: R=0.4053 | Gen=' FULL LICENSE of | A=A little golden key'\n",
      "step 7500: train loss 2.5706, val loss 8.7263, delta -0.1025 --------------------------\n",
      "RL iter 7500: R=0.4136 | Gen=' “O mouse—and salmon.’re sure very\n",
      "“—it� | A=She sees a White Rabbit speaking and notices it has a waistcoat-pocket and a watch.'\n",
      "RL iter 7550: R=0.4863 | Gen=' your state of the party wereter in both a great\n",
      " | A=\"Why is a raven like a writing-desk?\"'\n",
      "step 7600: train loss 2.5994, val loss 8.6671, delta -0.0592 --------------------------\n",
      "RL iter 7600: R=0.5246 | Gen=' perhaps you could not!� | A=\"Off with his head!\"'\n",
      "RL iter 7650: R=0.7451 | Gen=' perhaps youFirst or | A=The Cheshire Cat'\n",
      "step 7700: train loss 2.5609, val loss 8.7430, delta 0.0758 --------------------------\n",
      "RL iter 7700: R=0.5425 | Gen=' “so mustard way, whose\n",
      "isies,’t believe he produc | A=She sees a White Rabbit speaking and notices it has a waistcoat-pocket and a watch.'\n",
      "RL iter 7750: R=0.6655 | Gen=' receipt, and the rest of rock, “and among the | A=Absurd, whimsical, chaotic, and filled with nonsensical logic'\n",
      "step 7800: train loss 2.6401, val loss 8.5992, delta -0.1437 --------------------------\n",
      "RL iter 7800: R=0.5336 | Gen=' receipt, by the clock. “I think.”\n",
      "On this time Al | A=It changes unpredictably—she shrinks after drinking from a bottle and grows after eating a cake'\n",
      "RL iter 7850: R=0.7865 | Gen=' receipt, and after all: receipt, run away, and th | A=He presides over it and even writes down rules and evidence, despite the absurdity of the proceedings'\n",
      "step 7900: train loss 2.7236, val loss 8.8073, delta 0.2080 --------------------------\n",
      "RL iter 7900: R=0.6655 | Gen=' receipt, and the date on and her arm-box, and | A=Absurd, whimsical, chaotic, and filled with nonsensical logic'\n",
      "RL iter 7950: R=0.5608 | Gen=' receipt on the after all that stood hall, and aft | A=That she is changing and sometimes does not feel like the person she was at the start'\n",
      "step 8000: train loss 2.6602, val loss 8.8393, delta 0.0320 --------------------------\n",
      "RL iter 8000: R=0.7010 | Gen=' the hall, | A=She grows larger'\n",
      "RL iter 8050: R=0.8517 | Gen=' the hall, and howling appeared to the hall, rathe | A=She falls down a deep rabbit-hole into a strange world.'\n",
      "step 8100: train loss 2.6017, val loss 9.0976, delta 0.2582 --------------------------\n",
      "RL iter 8100: R=0.7414 | Gen=' the hall, and howling appeared to the hall, for a | A=He presides over it and even writes down rules and evidence, despite the absurdity of the proceedings'\n",
      "RL iter 8150: R=0.5916 | Gen=' the March Hare And she never saw\n",
      "was, and grinned | A=Live flamingos used as mallets, hedgehogs as balls, and soldiers doubling as arches'\n",
      "step 8200: train loss 2.6195, val loss 9.0381, delta -0.0594 --------------------------\n",
      "RL iter 8200: R=0.5884 | Gen=' the March Hare she never saw\n",
      "was, half neck kept  | A=It changes unpredictably—she shrinks after drinking from a bottle and grows after eating a cake'\n",
      "RL iter 8250: R=0.6991 | Gen=' the March Hare she never saw\n",
      "was, for ten alarm.  | A=Alice wakes up on the riverbank, realizing that her adventures in Wonderland were a dream'\n",
      "step 8300: train loss 2.5943, val loss 9.0097, delta -0.0285 --------------------------\n",
      "RL iter 8300: R=0.6059 | Gen=' the Foundation as quickly as she came upon this,  | A=It is organized to dry the party after they all fall into a pool of tears'\n",
      "RL iter 8350: R=0.5030 | Gen=' “because it goes\n",
      "stir, | A=He also acts as a herald during the trial'\n",
      "step 8400: train loss 2.5558, val loss 8.9032, delta -0.1065 --------------------------\n",
      "RL iter 8400: R=0.7010 | Gen=' the hall, | A=She grows larger'\n",
      "RL iter 8450: R=0.7303 | Gen=' the hall, and Seven how oftenand bit and four aga | A=He presides over it and even writes down rules and evidence, despite the absurdity of the proceedings'\n",
      "step 8500: train loss 2.5824, val loss 9.0180, delta 0.1148 --------------------------\n",
      "RL iter 8500: R=0.4973 | Gen=' the Foundation as\n",
      "easy.\n",
      "with each side of the | A=\"Why is a raven like a writing-desk?\"'\n",
      "RL iter 8550: R=0.7010 | Gen=' the hall, | A=She grows larger'\n",
      "step 8600: train loss 2.5132, val loss 8.9532, delta -0.0648 --------------------------\n",
      "RL iter 8600: R=0.8057 | Gen=' the Dormouse: | A=The Knave of Hearts'\n",
      "RL iter 8650: R=0.5030 | Gen=' the Dormouse: theormouse had put on: | A=The Project Gutenberg eBook of Alice’s Adventures in Wonderland'\n",
      "step 8700: train loss 2.4815, val loss 9.1356, delta 0.1825 --------------------------\n",
      "RL iter 8700: R=0.6575 | Gen=' the hall, as a good,\n",
      "Lory, of cardboard | A=She falls down a deep rabbit-hole into a strange world.'\n",
      "RL iter 8750: R=0.9529 | Gen=' the three gardeners | A=The Cheshire Cat'\n",
      "step 8800: train loss 2.4992, val loss 9.0627, delta -0.0729 --------------------------\n",
      "RL iter 8800: R=0.6805 | Gen=' the hall, as a good,\n",
      "Lory, of\n",
      " | A=She falls down a deep rabbit-hole into a strange world.'\n",
      "RL iter 8850: R=0.6961 | Gen=' the Foundation as\n",
      "easy.3, the F, kept a reasonabl | A=It is organized to dry the party after they all fall into a pool of tears'\n",
      "step 8900: train loss 2.5520, val loss 9.1838, delta 0.1211 --------------------------\n",
      "RL iter 8900: R=0.3164 | Gen=' the Foundation interrupted.gutenberg.6221541.3, k | A=Alice wakes up on the riverbank, realizing that her adventures in Wonderland were a dream'\n",
      "RL iter 8950: R=0.4613 | Gen=' the clock.g, the Mouse, kept the right paw: the g | A=It changes unpredictably—she shrinks after drinking from a bottle and grows after eating a cake'\n",
      "step 9000: train loss 2.5036, val loss 9.2033, delta 0.0195 --------------------------\n",
      "RL iter 9000: R=0.7268 | Gen=' the candle.\n",
      " | A=A little golden key'\n",
      "RL iter 9050: R=0.6534 | Gen=' the great relief.\n",
      "They were lying on his son sigh | A=It is chaotic, with everyone talking, running, and arguing without a clear order'\n",
      "step 9100: train loss 2.4690, val loss 9.0214, delta -0.1819 --------------------------\n",
      "RL iter 9100: R=0.7010 | Gen=' the hall, | A=She grows larger'\n",
      "RL iter 9150: R=0.5362 | Gen=' the great relief.\n",
      "They | A=Down the Rabbit-Hole'\n",
      "step 9200: train loss 2.4090, val loss 9.0705, delta 0.0492 --------------------------\n",
      "RL iter 9200: R=0.6617 | Gen=' the Queen, sitting him came flying down the shade | A=The Project Gutenberg eBook of Alice’s Adventures in Wonderland'\n",
      "RL iter 9250: R=0.5927 | Gen=' the Queen, sitting him came flying through genera | A=The Project Gutenberg eBook of Alice’s Adventures in Wonderland'\n",
      "step 9300: train loss 2.4262, val loss 9.0427, delta -0.0278 --------------------------\n",
      "RL iter 9300: R=0.5508 | Gen=' our hers—even.”\n",
      "“Certainly not quite follow a ver | A=It changes unpredictably—she shrinks after drinking from a bottle and grows after eating a cake'\n",
      "RL iter 9350: R=0.4069 | Gen=' the unfortunate guests, “chop\n",
      "unless _what | A=\"Why is a raven like a writing-desk?\"'\n",
      "step 9400: train loss 2.4332, val loss 9.0030, delta -0.0397 --------------------------\n",
      "RL iter 9400: R=0.7464 | Gen=' it exclaimed, promotion when she tried to nine\n",
      "be | A=Alice wakes up on the riverbank, realizing that her adventures in Wonderland were a dream'\n",
      "RL iter 9450: R=0.6595 | Gen=' she tried to nine\n",
      "began again, to ask | A=A set of verses that seem to have no clear meaning'\n",
      "step 9500: train loss 2.4531, val loss 8.9633, delta -0.0397 --------------------------\n",
      "RL iter 9500: R=0.6173 | Gen=' “Sentence went on the school in | A=He also acts as a herald during the trial'\n",
      "RL iter 9550: R=0.6555 | Gen=' the Foundation with another confusion of the Foun | A=Absurd, whimsical, chaotic, and filled with nonsensical logic'\n",
      "step 9600: train loss 2.4533, val loss 8.9636, delta 0.0003 --------------------------\n",
      "RL iter 9600: R=0.5238 | Gen=' “Oh! ever saw her waiting on the Conqueror\n",
      "great | A=A small bottle with a paper label that instructs her to drink it'\n",
      "RL iter 9650: R=0.6969 | Gen=' so madder in here, as\n",
      "the executioner honour, and | A=Live flamingos used as mallets, hedgehogs as balls, and soldiers doubling as arches'\n",
      "step 9700: train loss 2.5171, val loss 9.0215, delta 0.0579 --------------------------\n",
      "RL iter 9700: R=0.7587 | Gen=' but use of your applicable state of them taller,  | A=The Project Gutenberg eBook of Alice’s Adventures in Wonderland'\n",
      "RL iter 9750: R=0.5739 | Gen=' but checked himself; | A=It makes her shrink'\n",
      "step 9800: train loss 2.4120, val loss 9.0177, delta -0.0038 --------------------------\n",
      "RL iter 9800: R=0.3726 | Gen=' so madder\n",
      "people?” Just but checked himself; but  | A=It changes unpredictably—she shrinks after drinking from a bottle and grows after eating a cake'\n",
      "RL iter 9850: R=0.8882 | Gen=' not agree to the Queenpled.\n",
      "They were no chance o | A=It is organized to dry the party after they all fall into a pool of tears'\n",
      "step 9900: train loss 2.4969, val loss 9.0690, delta 0.0512 --------------------------\n",
      "RL iter 9900: R=0.7383 | Gen=' not them taller, and began full of write her one, | A=The Project Gutenberg eBook of Alice’s Adventures in Wonderland'\n",
      "RL iter 9950: R=0.5105 | Gen=' “lause,”,“the m” the you Mock Turtle yawn | A=She sees a White Rabbit speaking and notices it has a waistcoat-pocket and a watch.'\n",
      "step 10000: train loss 2.4753, val loss 9.0975, delta 0.0286 --------------------------\n",
      "RL iter 10000: R=0.4141 | Gen=' not, if I don’t guess of me one, when I don’ head | A=It changes unpredictably—she shrinks after drinking from a bottle and grows after eating a cake'\n",
      "RL iter 10050: R=0.3860 | Gen=' not, half no one, if then she had never could be  | A=He presides over it and even writes down rules and evidence, despite the absurdity of the proceedings'\n",
      "step 10100: train loss 2.4149, val loss 9.1309, delta 0.0334 --------------------------\n",
      "RL iter 10100: R=0.6480 | Gen=' she went on as well,\n",
      "but when she had never left  | A=A small bottle with a paper label that instructs her to drink it'\n",
      "RL iter 10150: R=0.7700 | Gen=' not,\n",
      "all one, “Back at the door in fact, they | A=It is organized to dry the party after they all fall into a pool of tears'\n",
      "step 10200: train loss 2.4306, val loss 9.0675, delta -0.0634 --------------------------\n",
      "RL iter 10200: R=0.8517 | Gen=' not,\n",
      "all one, then, “—that eggs, they live | A=It is organized to dry the party after they all fall into a pool of tears'\n",
      "RL iter 10250: R=0.4783 | Gen=' not, I don’t be impert\n",
      "there_ finish my own coura | A=It changes unpredictably—she shrinks after drinking from a bottle and grows after eating a cake'\n",
      "step 10300: train loss 2.4160, val loss 9.0634, delta -0.0041 --------------------------\n",
      "RL iter 10300: R=0.8235 | Gen=' she spoke, | A=She grows larger'\n",
      "RL iter 10350: R=0.7845 | Gen=' she spoke, | A=A small cake'\n",
      "step 10400: train loss 2.3402, val loss 9.1326, delta 0.0692 --------------------------\n",
      "RL iter 10400: R=0.7731 | Gen=' she spoke, in the March Hare had never left off t | A=That she is changing and sometimes does not feel like the person she was at the start'\n",
      "RL iter 10450: R=0.6928 | Gen=' she had never left off the). in all managing him  | A=That she is changing and sometimes does not feel like the person she was at the start'\n",
      "step 10500: train loss 2.3820, val loss 9.1425, delta 0.0099 --------------------------\n",
      "RL iter 10500: R=0.6864 | Gen=' she had been a remarkable sensation subject, so,  | A=That she is changing and sometimes does not feel like the person she was at the start'\n",
      "RL iter 10550: R=0.3839 | Gen=' she had been | A=A small cake'\n",
      "step 10600: train loss 2.3273, val loss 9.0899, delta -0.0526 --------------------------\n",
      "RL iter 10600: R=0.5441 | Gen=' not,\n",
      "all: and keep up a queer first: you, will ta | A=It is organized to dry the party after they all fall into a pool of tears'\n",
      "RL iter 10650: R=0.5550 | Gen=' and keep up. It very longed upon Alice, | A=A set of verses that seem to have no clear meaning'\n",
      "step 10700: train loss 2.3361, val loss 9.0769, delta -0.0129 --------------------------\n",
      "RL iter 10700: R=0.5061 | Gen=' a week: _at his Fender, makes it more; and did no | A=It changes unpredictably—she shrinks after drinking from a bottle and grows after eating a cake'\n",
      "RL iter 10750: R=0.5246 | Gen=' a queer first thing went | A=The Knave of Hearts'\n",
      "step 10800: train loss 2.3056, val loss 9.3168, delta 0.2399 --------------------------\n",
      "RL iter 10800: R=0.5608 | Gen=' not a great concert,\n",
      "because\n",
      "to the rose up | A=\"Why is a raven like a writing-desk?\"'\n",
      "RL iter 10850: R=0.7430 | Gen=' a remarkable sensation, until it does, but said n | A=That she is changing and sometimes does not feel like the person she was at the start'\n",
      "step 10900: train loss 2.3564, val loss 9.4286, delta 0.1118 --------------------------\n",
      "RL iter 10900: R=0.3171 | Gen=' an advantage, of course. | A=\"Off with his head!\"'\n",
      "RL iter 10950: R=0.4559 | Gen=' shut.\n",
      "“I won’t remember a large dig of a | A=It is chaotic, with everyone talking, running, and arguing without a clear order'\n",
      "step 11000: train loss 2.2960, val loss 9.3373, delta -0.0913 --------------------------\n",
      "RL iter 11000: R=0.7084 | Gen=' a large piece for a difficult question, “and\n",
      "mous | A=She falls down a deep rabbit-hole into a strange world.'\n",
      "RL iter 11050: R=0.6953 | Gen=' a dance. “He must be read that D | A=A set of verses that seem to have no clear meaning'\n",
      "step 11100: train loss 2.3081, val loss 9.3650, delta 0.0277 --------------------------\n",
      "RL iter 11100: R=0.5030 | Gen=' a dance.\n",
      "“Mine went on, I’ | A=She falls down a deep rabbit-hole into a strange world.'\n",
      "RL iter 11150: R=0.2242 | Gen=' all!” | A=The Cheshire Cat'\n",
      "step 11200: train loss 2.2367, val loss 9.3178, delta -0.0472 --------------------------\n",
      "RL iter 11200: R=0.6232 | Gen=' so\n",
      "earsself, as she went on eagerly from her coax | A=A small bottle with a paper label that instructs her to drink it'\n",
      "RL iter 11250: R=0.6162 | Gen=' a dance.\n",
      "Alice caught the see it again, as she | A=She falls down a deep rabbit-hole into a strange world.'\n",
      "step 11300: train loss 2.2718, val loss 9.3693, delta 0.0515 --------------------------\n",
      "RL iter 11300: R=0.5782 | Gen=' a table.\n",
      "Alice caught the come up like a large.\n",
      "T | A=That she is changing and sometimes does not feel like the person she was at the start'\n",
      "RL iter 11350: R=0.5229 | Gen=' a table.\n",
      "The times longed up a large,\n",
      "makes them  | A=He presides over it and even writes down rules and evidence, despite the absurdity of the proceedings'\n",
      "step 11400: train loss 2.2559, val loss 9.4677, delta 0.0983 --------------------------\n",
      "RL iter 11400: R=0.4169 | Gen=' they chin | A=Lewis Carroll'\n",
      "RL iter 11450: R=0.3485 | Gen=' adoor, CONSEQUENTIAL,\n",
      "“What11 | A=A small bottle with a paper label that instructs her to drink it'\n",
      "step 11500: train loss 2.2000, val loss 9.3702, delta -0.0974 --------------------------\n",
      "RL iter 11500: R=0.4169 | Gen=' they chin | A=Lewis Carroll'\n",
      "RL iter 11550: R=0.4169 | Gen=' they chin | A=Lewis Carroll'\n",
      "step 11600: train loss 2.1359, val loss 9.4277, delta 0.0575 --------------------------\n",
      "RL iter 11600: R=0.3979 | Gen=' this,\n",
      "small.\n",
      " thought\n",
      "she\n",
      "number caught the | A=Absurd, whimsical, chaotic, and filled with nonsensical logic'\n",
      "RL iter 11650: R=0.5949 | Gen=' you know.”\n",
      "And so, and in | A=A set of verses that seem to have no clear meaning'\n",
      "step 11700: train loss 2.1930, val loss 9.3582, delta -0.0695 --------------------------\n",
      "RL iter 11700: R=0.3432 | Gen=' this was\n",
      " search\n",
      " work not\n",
      "pan softly after the\n",
      " | A=Absurd, whimsical, chaotic, and filled with nonsensical logic'\n",
      "RL iter 11750: R=0.5855 | Gen=' she did not taste theirs, as much\n",
      "appeared. Theti | A=Alice wakes up on the riverbank, realizing that her adventures in Wonderland were a dream'\n",
      "step 11800: train loss 2.2150, val loss 9.3471, delta -0.0111 --------------------------\n",
      "RL iter 11800: R=0.4656 | Gen='\n",
      "either question, which wasn’t you know | A=A set of verses that seem to have no clear meaning'\n",
      "RL iter 11850: R=0.6218 | Gen='\n",
      "either question, then, but said to fall.\n",
      "“I won’t | A=He presides over it and even writes down rules and evidence, despite the absurdity of the proceedings'\n",
      "step 11900: train loss 2.1933, val loss 9.3397, delta -0.0074 --------------------------\n",
      "RL iter 11900: R=0.2350 | Gen=' March Hare.\n",
      "Here, till the White Rabbit whispered | A=It is chaotic, with everyone talking, running, and arguing without a clear order'\n",
      "RL iter 11950: R=0.4169 | Gen=' they chin | A=Lewis Carroll'\n",
      "step 12000: train loss 2.1071, val loss 9.3552, delta 0.0155 --------------------------\n",
      "RL iter 12000: R=0.5673 | Gen=' March Hare; and a great Hare said to fall. “Now I | A=He presides over it and even writes down rules and evidence, despite the absurdity of the proceedings'\n",
      "RL iter 12050: R=0.8014 | Gen=' same, until a Caterpillar down your see he had be | A=That she is changing and sometimes does not feel like the person she was at the start'\n",
      "step 12100: train loss 2.2185, val loss 9.3323, delta -0.0229 --------------------------\n",
      "RL iter 12100: R=0.6848 | Gen=' at first _not_ a week; and nonsense:, 2021\n",
      "Langua | A=It changes unpredictably—she shrinks after drinking from a bottle and grows after eating a cake'\n",
      "RL iter 12150: R=0.2916 | Gen=' on, | A=Lewis Carroll'\n",
      "step 12200: train loss 2.1652, val loss 9.4745, delta 0.1422 --------------------------\n",
      "RL iter 12200: R=0.7643 | Gen=' at first three dates on.\n",
      "Here, and a goodbing. He | A=It is chaotic, with everyone talking, running, and arguing without a clear order'\n",
      "RL iter 12250: R=0.5639 | Gen=' at first three dates on; and nonsense say altoget | A=Live flamingos used as mallets, hedgehogs as balls, and soldiers doubling as arches'\n",
      "step 12300: train loss 2.2409, val loss 9.3969, delta -0.0776 --------------------------\n",
      "RL iter 12300: R=0.6696 | Gen=' at first three dates on; also Come on;’\n",
      "the March | A=It is organized to dry the party after they all fall into a pool of tears'\n",
      "RL iter 12350: R=0.8517 | Gen=' at\n",
      "all that it would | A=Down the Rabbit-Hole'\n",
      "step 12400: train loss 2.2217, val loss 9.4346, delta 0.0377 --------------------------\n",
      "RL iter 12400: R=0.5680 | Gen=' a Caterpillar down on. Alice\n",
      "“Consider your verdi | A=That she is changing and sometimes does not feel like the person she was at the start'\n",
      "RL iter 12450: R=0.6465 | Gen=' at\n",
      "all that she could make out what\n",
      "wanted to hav | A=It is organized to dry the party after they all fall into a pool of tears'\n",
      "step 12500: train loss 2.2472, val loss 9.4555, delta 0.0209 --------------------------\n",
      "RL iter 12500: R=0.2142 | Gen=' at\n",
      "all | A=She grows larger'\n",
      "RL iter 12550: R=0.5088 | Gen=' at\n",
      "all that she could leave off to her hair\n",
      " | A=\"Why is a raven like a writing-desk?\"'\n",
      "step 12600: train loss 2.1250, val loss 9.4899, delta 0.0345 --------------------------\n",
      "RL iter 12600: R=0.2751 | Gen=' “That | A=A small cake'\n",
      "RL iter 12650: R=0.3693 | Gen=' at\n",
      "all that:—”\n",
      "“It“I don | A=On a mix of nonsensical evidence and arbitrary rules, such as Rule Forty-two'\n",
      "step 12700: train loss 2.1046, val loss 9.4043, delta -0.0856 --------------------------\n",
      "RL iter 12700: R=0.7025 | Gen=' at\n",
      "all that had been anything near, and\n",
      "she | A=The Project Gutenberg eBook of Alice’s Adventures in Wonderland'\n",
      "RL iter 12750: R=0.4476 | Gen=' as strange, “I breathe is a _are_ _are | A=A small bottle with a paper label that instructs her to drink it'\n",
      "step 12800: train loss 2.1316, val loss 9.4503, delta 0.0460 --------------------------\n",
      "RL iter 12800: R=0.8288 | Gen=' as an opportunity of showing off.\n",
      "Here, and marke | A=She sees a White Rabbit speaking and notices it has a waistcoat-pocket and a watch.'\n",
      "RL iter 12850: R=0.7671 | Gen=' as an opportunity of showing off.\n",
      "Here, and marke | A=She sees a White Rabbit speaking and notices it has a waistcoat-pocket and a watch.'\n",
      "step 12900: train loss 2.1497, val loss 9.5292, delta 0.0788 --------------------------\n",
      "RL iter 12900: R=0.3741 | Gen=' as an opportunity of showing off.\n",
      "Here, and marke | A=She falls down a deep rabbit-hole into a strange world.'\n",
      "RL iter 12950: R=0.7505 | Gen=' as an opportunity of showing off.\n",
      "Here, and marke | A=Alice wakes up on the riverbank, realizing that her adventures in Wonderland were a dream'\n",
      "step 13000: train loss 2.1332, val loss 9.5381, delta 0.0089 --------------------------\n",
      "RL iter 13000: R=0.5263 | Gen=' as an opportunity of showing off.\n",
      "Here, and marke | A=He presides over it and even writes down rules and evidence, despite the absurdity of the proceedings'\n",
      "RL iter 13050: R=0.5451 | Gen=' as an opportunity | A=She grows larger'\n",
      "step 13100: train loss 2.0392, val loss 9.6227, delta 0.0846 --------------------------\n",
      "RL iter 13100: R=0.5030 | Gen='\n",
      "in\n",
      "one,\n",
      " | A=Down the Rabbit-Hole'\n",
      "RL iter 13150: R=0.6660 | Gen=' as an opportunity of showing off.\n",
      "Here,\n",
      "makes the | A=It is organized to dry the party after they all fall into a pool of tears'\n",
      "step 13200: train loss 2.0623, val loss 9.6499, delta 0.0272 --------------------------\n",
      "RL iter 13200: R=0.4373 | Gen=' as strange\n",
      "off, as an opportunity of showing off. | A=A small bottle with a paper label that instructs her to drink it'\n",
      "RL iter 13250: R=0.6408 | Gen=' as an opportunity of showing off.\n",
      "� | A=He also acts as a herald during the trial'\n",
      "step 13300: train loss 2.0163, val loss 9.6690, delta 0.0191 --------------------------\n",
      "RL iter 13300: R=0.6408 | Gen=' as an opportunity of showing off.\n",
      "� | A=He also acts as a herald during the trial'\n",
      "RL iter 13350: R=0.5717 | Gen=' as an opportunity of showing | A=The Knave of Hearts'\n",
      "step 13400: train loss 1.9604, val loss 9.6365, delta -0.0325 --------------------------\n",
      "RL iter 13400: R=0.6408 | Gen=' as an opportunity of showing off.\n",
      "� | A=He also acts as a herald during the trial'\n",
      "RL iter 13450: R=0.2942 | Gen=' as an opportunity of showing off.\n",
      "“Please you,� | A=A small bottle with a paper label that instructs her to drink it'\n",
      "step 13500: train loss 1.9490, val loss 9.7009, delta 0.0644 --------------------------\n",
      "RL iter 13500: R=0.7568 | Gen=' as he spoke: as an opportunity, and\n",
      "“Begin your,  | A=It changes unpredictably—she shrinks after drinking from a bottle and grows after eating a cake'\n",
      "RL iter 13550: R=0.3373 | Gen=' as he spoke.)\n",
      "“Begin your,“I wasn’ | A=On a mix of nonsensical evidence and arbitrary rules, such as Rule Forty-two'\n",
      "step 13600: train loss 2.0086, val loss 9.8530, delta 0.1522 --------------------------\n",
      "RL iter 13600: R=0.7971 | Gen=' as he spoke | A=She grows larger'\n",
      "RL iter 13650: R=0.5220 | Gen=' as he spoke.)\n",
      "“Begin your, and,” the Queen turned | A=Live flamingos used as mallets, hedgehogs as balls, and soldiers doubling as arches'\n",
      "step 13700: train loss 1.9765, val loss 9.8750, delta 0.0219 --------------------------\n",
      "RL iter 13700: R=1.0000 | Gen=' as he spoke.) | A=It makes her shrink'\n",
      "RL iter 13750: R=0.7859 | Gen=' as he spoke.) | A=A little golden key'\n",
      "step 13800: train loss 1.9193, val loss 9.9139, delta 0.0389 --------------------------\n",
      "RL iter 13800: R=0.5431 | Gen=' as he spoke.)\n",
      "“Begin your, and, and, I did,’ll | A=Live flamingos used as mallets, hedgehogs as balls, and soldiers doubling as arches'\n",
      "RL iter 13850: R=0.5173 | Gen=' as heipped\n",
      "made | A=The Knave of Hearts'\n",
      "step 13900: train loss 1.9307, val loss 9.7162, delta -0.1977 --------------------------\n",
      "RL iter 13900: R=0.4875 | Gen=' as I could sound as it\n",
      "can’m finish your story!”\n",
      " | A=It changes unpredictably—she shrinks after drinking from a bottle and grows after eating a cake'\n",
      "RL iter 13950: R=0.3981 | Gen=' as I could not venture to\n",
      "near\n",
      "she felt with bein | A=It is organized to dry the party after they all fall into a pool of tears'\n",
      "step 14000: train loss 1.9905, val loss 9.7686, delta 0.0524 --------------------------\n",
      "RL iter 14000: R=0.4762 | Gen=' as I could not venture to\n",
      "near\n",
      "she felt to she lo | A=Live flamingos used as mallets, hedgehogs as balls, and soldiers doubling as arches'\n",
      "RL iter 14050: R=0.5853 | Gen=' as she could not, could not venture to it to\n",
      "near | A=She falls down a deep rabbit-hole into a strange world.'\n",
      "step 14100: train loss 1.9569, val loss 9.8750, delta 0.1064 --------------------------\n",
      "RL iter 14100: R=0.7242 | Gen=' as it.” (she heard a Caterpillar | A=A set of verses that seem to have no clear meaning'\n",
      "RL iter 14150: R=0.6262 | Gen=' as it.” (she heard a Caterpillarbefore, that did | A=The text includes the full Project Gutenberg license and terms of use for the ebook'\n",
      "step 14200: train loss 1.9666, val loss 9.8605, delta -0.0145 --------------------------\n",
      "RL iter 14200: R=0.3470 | Gen=' as it.” | A=The Knave of Hearts'\n",
      "RL iter 14250: R=0.8023 | Gen=' as she swam, that did not, or she did not venture | A=Alice wakes up on the riverbank, realizing that her adventures in Wonderland were a dream'\n",
      "step 14300: train loss 2.0687, val loss 9.8371, delta -0.0235 --------------------------\n",
      "RL iter 14300: R=0.4447 | Gen=' as she swam | A=A little golden key'\n",
      "RL iter 14350: R=0.8307 | Gen=' as she swam, it | A=Down the Rabbit-Hole'\n",
      "step 14400: train loss 1.9456, val loss 9.8934, delta 0.0563 --------------------------\n",
      "RL iter 14400: R=0.5590 | Gen=' as heipped\n",
      "made up.” she began again, and it away | A=On a mix of nonsensical evidence and arbitrary rules, such as Rule Forty-two'\n",
      "RL iter 14450: R=0.6583 | Gen=' as if she swam, it away again.\n",
      "“He are, no busine | A=It changes unpredictably—she shrinks after drinking from a bottle and grows after eating a cake'\n",
      "step 14500: train loss 2.0247, val loss 9.9200, delta 0.0266 --------------------------\n",
      "RL iter 14500: R=0.6579 | Gen=' as if they swam, as if he again: as if\n",
      "_ she | A=It is chaotic, with everyone talking, running, and arguing without a clear order'\n",
      "RL iter 14550: R=0.5802 | Gen=' as if he again | A=The Cheshire Cat'\n",
      "step 14600: train loss 1.9362, val loss 10.0086, delta 0.0886 --------------------------\n",
      "RL iter 14600: R=0.6757 | Gen=' as if he again: as | A=Down the Rabbit-Hole'\n",
      "RL iter 14650: R=0.6991 | Gen=' as if he again: as if\n",
      "_ cake, for\n",
      "the Duchess | A=The text includes the full Project Gutenberg license and terms of use for the ebook'\n",
      "step 14700: train loss 1.9394, val loss 10.0879, delta 0.0793 --------------------------\n",
      "RL iter 14700: R=0.7089 | Gen=' as if she next | A=The Cheshire Cat'\n",
      "RL iter 14750: R=0.7652 | Gen=' as if she next | A=A little golden key'\n",
      "step 14800: train loss 1.8506, val loss 10.0405, delta -0.0474 --------------------------\n",
      "RL iter 14800: R=0.5687 | Gen=' as she next | A=A small cake'\n",
      "RL iter 14850: R=0.4396 | Gen=' as she next, as she next again: as before, she\n",
      "ha | A=He presides over it and even writes down rules and evidence, despite the absurdity of the proceedings'\n",
      "step 14900: train loss 1.8719, val loss 10.1344, delta 0.0939 --------------------------\n",
      "RL iter 14900: R=0.5661 | Gen=' as she swam, lying down on again: as she\n",
      "half-fra | A=It is chaotic, with everyone talking, running, and arguing without a clear order'\n",
      "RL iter 14950: R=0.5470 | Gen=' as she swam, lying down on again: as she\n",
      "half-up | A=It is chaotic, with everyone talking, running, and arguing without a clear order'\n",
      "step 15000: train loss 1.8331, val loss 10.2313, delta 0.0969 --------------------------\n",
      "RL iter 15000: R=0.7532 | Gen=' to she next! | A=The Cheshire Cat'\n",
      "RL iter 15050: R=0.5418 | Gen=' to she next! Oh! Oh! I think I can | A=The Project Gutenberg eBook of Alice’s Adventures in Wonderland'\n",
      "step 15100: train loss 1.8708, val loss 10.1855, delta -0.0458 --------------------------\n",
      "RL iter 15100: R=0.4248 | Gen=' to she next! Oh! I wish you please down here? I t | A=Live flamingos used as mallets, hedgehogs as balls, and soldiers doubling as arches'\n",
      "RL iter 15150: R=0.6886 | Gen=' to she next! | A=A little golden key'\n",
      "step 15200: train loss 1.8209, val loss 10.1851, delta -0.0004 --------------------------\n",
      "RL iter 15200: R=0.7089 | Gen=' to she next! Oh! I another rush at the door.\n",
      " | A=A small bottle with a paper label that instructs her to drink it'\n",
      "RL iter 15250: R=0.7089 | Gen=' to she next! Oh! | A=\"Off with his head!\"'\n",
      "step 15300: train loss 1.7962, val loss 10.2694, delta 0.0843 --------------------------\n",
      "RL iter 15300: R=0.7089 | Gen=' to she next! Oh! being quite enough; I give in co | A=It is organized to dry the party after they all fall into a pool of tears'\n",
      "RL iter 15350: R=0.6133 | Gen=' as she next out again.\n",
      "”\n",
      "Alice had for the air. | A=It is chaotic, with everyone talking, running, and arguing without a clear order'\n",
      "step 15400: train loss 1.9186, val loss 10.1291, delta -0.1403 --------------------------\n",
      "RL iter 15400: R=0.5687 | Gen=' as she next | A=A small cake'\n",
      "RL iter 15450: R=0.5590 | Gen=' as she next out\n",
      "anything but it again.\n",
      "”\n",
      "� | A=The text includes the full Project Gutenberg license and terms of use for the ebook'\n",
      "step 15500: train loss 1.8480, val loss 10.1170, delta -0.0120 --------------------------\n",
      "RL iter 15500: R=0.4906 | Gen=' I a lesson- | A=The Cheshire Cat'\n",
      "RL iter 15550: R=0.6383 | Gen=' as she next | A=She grows larger'\n",
      "step 15600: train loss 1.7692, val loss 10.1940, delta 0.0770 --------------------------\n",
      "RL iter 15600: R=0.8077 | Gen=' as she next! | A=It makes her shrink'\n",
      "RL iter 15650: R=0.3962 | Gen=' as she next! Oh! being invited,” said it.\n",
      "“It� | A=He presides over it and even writes down rules and evidence, despite the absurdity of the proceedings'\n",
      "step 15700: train loss 1.7679, val loss 10.3105, delta 0.1165 --------------------------\n",
      "RL iter 15700: R=0.5687 | Gen=' as she next | A=A small cake'\n",
      "RL iter 15750: R=0.6021 | Gen=' as she next! Oh\n",
      "”\n",
      "what\n",
      " | A=A set of verses that seem to have no clear meaning'\n",
      "step 15800: train loss 1.7836, val loss 10.3014, delta -0.0092 --------------------------\n",
      "RL iter 15800: R=0.5133 | Gen=' as she next! Oh\n",
      "”\n",
      "what\n",
      "what\n",
      "”\n",
      "� | A=Alice wakes up on the riverbank, realizing that her adventures in Wonderland were a dream'\n",
      "RL iter 15850: R=0.7177 | Gen=' as she next! Oh writing up by� | A=He also acts as a herald during the trial'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 277\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;66;03m# Supervised learning step\u001b[39;00m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m%\u001b[39m eval_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28miter\u001b[39m \u001b[38;5;241m==\u001b[39m max_iters\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 277\u001b[0m         losses \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28miter\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, delta \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m-\u001b[39mtest_loss_values[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m --------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    279\u001b[0m         epoch_count\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28miter\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\yaniv\\OneDrive\\python-flask\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 106\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[0;32m    105\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(split)\n\u001b[1;32m--> 106\u001b[0m     _, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m    108\u001b[0m out[split] \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\yaniv\\OneDrive\\python-flask\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yaniv\\OneDrive\\python-flask\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[30], line 185\u001b[0m, in \u001b[0;36mBigramLanguageModel.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m    183\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(torch\u001b[38;5;241m.\u001b[39marange(T, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong))\n\u001b[0;32m    184\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb\n\u001b[1;32m--> 185\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[0;32m    187\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "File \u001b[1;32mc:\\Users\\yaniv\\OneDrive\\python-flask\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yaniv\\OneDrive\\python-flask\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\yaniv\\OneDrive\\python-flask\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\yaniv\\OneDrive\\python-flask\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yaniv\\OneDrive\\python-flask\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[30], line 165\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 165\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffwd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\yaniv\\OneDrive\\python-flask\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yaniv\\OneDrive\\python-flask\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[30], line 140\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    141\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out))\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\yaniv\\OneDrive\\python-flask\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yaniv\\OneDrive\\python-flask\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[30], line 126\u001b[0m, in \u001b[0;36mHead.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    124\u001b[0m q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(x)\n\u001b[0;32m    125\u001b[0m wei \u001b[38;5;241m=\u001b[39m q \u001b[38;5;241m@\u001b[39m k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m (C \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m--> 126\u001b[0m wei \u001b[38;5;241m=\u001b[39m wei\u001b[38;5;241m.\u001b[39mmasked_fill(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtril\u001b[49m[:T,:T]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    127\u001b[0m wei \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(wei, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    128\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(wei)\n",
      "File \u001b[1;32mc:\\Users\\yaniv\\OneDrive\\python-flask\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1927\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1922\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[1;32m-> 1927\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m   1928\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m   1929\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from difflib import SequenceMatcher\n",
    "import matplotlib.pyplot as plt\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "\n",
    "# --------------------- Hyperparameters ---------------------\n",
    "batch_size = 16      # sequences per batch\n",
    "block_size = 32      # context length\n",
    "max_iters = 20000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'  # force CPU for simplicity\n",
    "eval_iters = 500\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "reward_interval = 50       # how often to run reward-based update\n",
    "reward_coef = 1.0          # weight of RL objective in combined loss\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "ce_loss_min = float('inf')\n",
    "ce_loss_max = 0.0\n",
    "\n",
    "# --------------------- Data Preparation ---------------------\n",
    "# currt_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# File paths\n",
    "input_pdf = \"datasets/alice.pdf\"\n",
    "qa_file   = 'datasets/alice_qa.txt'\n",
    "\n",
    "# Step 1. Extract text from the input PDF\n",
    "extracted_pages = []\n",
    "with pdfplumber.open(input_pdf) as pdf:\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text()\n",
    "        extracted_pages.append(text)\n",
    "alice_book = \" \".join(extracted_pages)\n",
    "\n",
    "# Step 2. Load Q&A pairs\n",
    "def load_qa_pairs(path):\n",
    "    pairs = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read().strip().split('\\n\\n')\n",
    "    for block in content:\n",
    "        if block.startswith('Q:') and '\\nA:' in block:\n",
    "            q,a = block.split('\\nA:')\n",
    "            question = q[len('Q:'):].strip()\n",
    "            answer   = a.strip()\n",
    "            pairs.append((question, answer))\n",
    "    return pairs\n",
    "\n",
    "qa_pairs = load_qa_pairs(qa_file)\n",
    "\n",
    "# Combine text and build vocabulary\n",
    "with open(qa_file, 'r', encoding='utf-8') as f:\n",
    "    qa_data = f.read()\n",
    "text = alice_book\n",
    "# chars = sorted(list(set(text + qa_data)))\n",
    "# vocab_size = len(chars)\n",
    "# stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "# itos = { i:ch for i,ch in enumerate(chars) }\n",
    "# encode = lambda s: [stoi[c] for c in s]\n",
    "# decode = lambda l: ''.join(itos[i] for i in l)\n",
    "\n",
    "# # Create train/val splits for supervised data\n",
    "# data = torch.tensor(encode(alice_book), dtype=torch.long)\n",
    "# n = int(0.9 * len(data))\n",
    "# train_data = data[:n]\n",
    "# test_data  = data[n:]\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "data_enc = np.array(enc.encode_ordinary(alice_book), dtype=np.uint16)  # Use uint16 to save space\n",
    "qa_data_enc = np.array(enc.encode_ordinary(qa_data), dtype=np.uint16)  # Use uint16 to save space\n",
    "n = int(0.9 * len(data_enc))\n",
    "m = int(0.9 * len(qa_data_enc))\n",
    "train_data_np = data_enc # np.concatenate((data[:n], qa_data[:m]))\n",
    "test_data_np = qa_data_enc # np.concatenate((data[n:], qa_data[m:]))\n",
    "train_data = torch.from_numpy(train_data_np).to(device).long()\n",
    "test_data = torch.from_numpy(test_data_np).to(device).long()\n",
    "vocab_size = enc.n_vocab\n",
    "# torch.from_numpy = lambda arr: torch.tensor(arr, dtype=torch.long)\n",
    "\n",
    "def get_batch(split):\n",
    "    data_split = train_data if split=='train' else test_data\n",
    "    ix = torch.randint(len(data_split) - block_size, (batch_size,))\n",
    "    x = torch.stack([data_split[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data_split[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ('train','val'):\n",
    "        losses = torch.zeros(eval_iters, device=device)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss\n",
    "        out[split] = losses.mean().item()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# --------------------- Model Definition ---------------------\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2,-1) * (C ** -0.5)\n",
    "        wei = wei.masked_fill(self.tril[:T,:T]==0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table    = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # ensure seq length ≤ block_size\n",
    "        idx = idx[:, -block_size:]\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device, dtype=torch.long))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        B, T, V = logits.shape\n",
    "        loss = F.cross_entropy(logits.view(B*T, V), targets.view(B*T))\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "        return idx\n",
    "\n",
    "# Instantiate model and optimizer\n",
    "model = BigramLanguageModel().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Reward function: simple exact match\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "def compute_reward(gen: str, ref: str) -> float:\n",
    "    global ce_loss_min, ce_loss_max\n",
    "    # 1) similarity ∈ [0,1]\n",
    "    s = SequenceMatcher(None, gen.strip(), ref.strip()).ratio()\n",
    "\n",
    "    # 2) build raw scores (we treat them directly as 'logits')\n",
    "    #    larger s → higher score for class '1' (match)\n",
    "    logits = torch.tensor([[s, 1.0 - s]], dtype=torch.float32)\n",
    "\n",
    "    # 3) true label: 1 if exact match, else 0\n",
    "    label = 1 if gen.strip() == ref.strip() else 0\n",
    "    target = torch.tensor([label], dtype=torch.long)\n",
    "\n",
    "    # compute cross-entropy loss\n",
    "    ce_loss = F.cross_entropy(logits, target, reduction='none')[0]\n",
    "\n",
    "    # update observed min/max of ce_loss\n",
    "    ce_val = ce_loss.item()\n",
    "    ce_loss_min = min(ce_loss_min, ce_val)\n",
    "    ce_loss_max = max(ce_loss_max, ce_val)\n",
    "\n",
    "    # scale reward so that ce_loss_min→1.0 and ce_loss_max→0.0\n",
    "    span = ce_loss_max - ce_loss_min if ce_loss_max > ce_loss_min else 1.0\n",
    "    reward = (ce_loss_max - ce_loss) / span\n",
    "\n",
    "    return float(reward)\n",
    "\n",
    "# RL training step\n",
    "\n",
    "def rl_update(question, reference):\n",
    "    # prepare prompt\n",
    "    prompt = f\"Q: {question}\\nA:\"\n",
    "    idx = torch.tensor([enc.encode(prompt)], device=device)\n",
    "    # run forward to collect logprobs\n",
    "    logits_seq, _ = model(idx)\n",
    "    log_probs = []\n",
    "    generated = []\n",
    "    # sample tokens\n",
    "    for _ in range(len(enc.encode(reference))):\n",
    "        logits = logits_seq[:, -1, :]\n",
    "        dist = Categorical(logits=logits)\n",
    "        token = dist.sample()\n",
    "        log_prob = dist.log_prob(token)\n",
    "        log_probs.append(log_prob)\n",
    "        generated.append(token.item())\n",
    "        idx = torch.cat([idx, token.unsqueeze(0)], dim=1)\n",
    "        logits_seq, _ = model(idx)\n",
    "    gen_str = enc.decode(generated)\n",
    "    r = compute_reward(gen_str, reference)\n",
    "    # policy gradient loss\n",
    "    log_probs = torch.stack(log_probs).sum()\n",
    "    loss = -reward_coef * r * log_probs\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return gen_str, r\n",
    "\n",
    "# Create empty loss lists to track values\n",
    "train_loss_values = [0.0]\n",
    "test_loss_values = [0.0]\n",
    "epoch_count = []\n",
    "\n",
    "# ----------------------- Training Loop -----------------------\n",
    "for iter in range(max_iters):\n",
    "    # Supervised learning step\n",
    "    if iter % eval_interval == 0 or iter == max_iters-1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, delta {(losses['val']-test_loss_values[-1]):.4f} --------------------------\")\n",
    "        epoch_count.append(iter)\n",
    "        train_loss_values.append(losses['train'])\n",
    "        test_loss_values.append(losses['val'])\n",
    "    xb, yb = get_batch('train')\n",
    "    _, s_loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    s_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Reinforcement update intermittently\n",
    "    if iter % reward_interval == 0:\n",
    "        # sample a random QA pair\n",
    "        q, a = qa_pairs[torch.randint(len(qa_pairs), (1,)).item()]\n",
    "        gen, rew = rl_update(q, a)\n",
    "        print(f\"RL iter {iter}: R={rew:.4f} | Gen='{gen[:50]} | A={a}'\")\n",
    "\n",
    "# Inference remains unchanged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c967320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x21525cbe450>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curves\n",
    "plt.scatter(epoch_count, train_loss_values[1:], label=\"Train loss\", s=4)\n",
    "plt.scatter(epoch_count, test_loss_values[1:], label=\"Test loss\", s=4)\n",
    "plt.title(\"Training and test loss curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "697b5c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 1. Create models directory\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 2. Create model save path\n",
    "MODEL_NAME = \"test_model_2.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2427d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save the model state dict\n",
    "import os\n",
    "if not os.path.exists(MODEL_SAVE_PATH):\n",
    "  print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "  torch.save(obj=model.state_dict(), # only saving the state_dict() only saves the models learned parameters\n",
    "            f=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4208371c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: models\\test_model_2.pth on device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BigramLanguageModel(\n",
       "  (token_embedding_table): Embedding(50257, 64)\n",
       "  (position_embedding_table): Embedding(32, 64)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Block(\n",
       "      (sa): MultiHeadAttention(\n",
       "        (heads): ModuleList(\n",
       "          (0-3): 4 x Head(\n",
       "            (key): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (query): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (value): Linear(in_features=64, out_features=16, bias=False)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ffwd): FeedForward(\n",
       "        (net): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=256, out_features=64, bias=True)\n",
       "          (3): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (ln1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (ln2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=64, out_features=50257, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model for evaluation\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Loading model from: {MODEL_SAVE_PATH}\", \"on device:\", device)\n",
    "save_model = BigramLanguageModel()\n",
    "save_model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=device))\n",
    "model = save_model\n",
    "model.to(device)\n",
    "model.eval() # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0107bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, max_len=100):\n",
    "    prompt = f\"Q: {question}\\nA:\"\n",
    "    idx = torch.tensor([enc.encode(prompt)], device=device)\n",
    "    out = model.generate(idx, max_new_tokens=max_len)[0].tolist()\n",
    "    return enc.decode(out[len(enc.encode(prompt)):]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd0c61bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the first chapter of the book\n",
      "Response: four times there are oldroom, but to keep its paws.\n",
      "“There was an individual Project Gutenberg” the only difficulty got. “Howly.\n",
      "” she mentioned herself when you wish you they were long Father William!\n",
      "said, _is_\n",
      "here_ a away. “Do that makes the him) be mind cut do when she spoke,” said pig, who instantly in a he began at me grow up by a king.\n",
      "“\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the first chapter of the book\" # \"What is the name of the cat in Alice's Adventures in Wonderland?\"\n",
    "print(f\"Question: {query}\")\n",
    "response = answer_question(query)\n",
    "print(f\"Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
