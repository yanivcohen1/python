{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc135a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# ---------- Custom Output Class ----------\n",
    "class QuestionAnsweringModelOutput:\n",
    "    def __init__(self, loss=None, start_logits=None, end_logits=None, hidden_states=None, attentions=None, reasoning_logits=None):\n",
    "        self.loss = loss\n",
    "        self.start_logits = start_logits\n",
    "        self.end_logits = end_logits\n",
    "        self.hidden_states = hidden_states\n",
    "        self.attentions = attentions\n",
    "        self.reasoning_logits = reasoning_logits\n",
    "\n",
    "# ---------- Custom BERT Implementation ----------\n",
    "class BertConfigCustom:\n",
    "    def __init__(self,\n",
    "                 vocab_size=30522,\n",
    "                 hidden_size=768,\n",
    "                 num_hidden_layers=12,\n",
    "                 num_attention_heads=12,\n",
    "                 intermediate_size=3072,\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 attention_probs_dropout_prob=0.1,\n",
    "                 max_position_embeddings=512,\n",
    "                 type_vocab_size=2,\n",
    "                 reasoning_vocab_size=3):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.reasoning_vocab_size = reasoning_vocab_size\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        words = self.word_embeddings(input_ids)\n",
    "        positions = self.position_embeddings(position_ids)\n",
    "        device = self.token_type_embeddings.weight.device\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        types = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = words + positions + types\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        return self.dropout(embeddings)\n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\"Hidden size must be divisible by number of heads\")\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.hidden_size // self.num_heads\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.key   = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.value = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "        def transpose(x):\n",
    "            return x.view(batch_size, seq_len, self.num_heads, self.head_dim) \\\n",
    "                    .permute(0, 2, 1, 3)\n",
    "\n",
    "        # Query (Q) – represents what this token is looking for.\n",
    "        # Key (K) – represents what this token contains.\n",
    "        # Value (V) – represents the actual content or information of the token.\n",
    "        Q = transpose(self.query(hidden_states))\n",
    "        K = transpose(self.key(hidden_states))\n",
    "        V = transpose(self.value(hidden_states))\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2))\n",
    "        scores = scores / math.sqrt(self.head_dim)\n",
    "        if attention_mask is not None:\n",
    "            scores = scores + attention_mask\n",
    "        probs = torch.softmax(scores, dim=-1)\n",
    "        probs = self.dropout(probs)\n",
    "\n",
    "        context = torch.matmul(probs, V)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous()\n",
    "        return context.view(batch_size, seq_len, -1)\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden = self.dense(hidden_states)\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.LayerNorm(hidden + input_tensor)\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        self_output = self.self(hidden_states, attention_mask)\n",
    "        return self.output(self_output, hidden_states)\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.intermediate_act_fn = nn.GELU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        return self.intermediate_act_fn(self.dense(hidden_states))\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden = self.dense(hidden_states)\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.LayerNorm(hidden + input_tensor)\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = BertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        return self.output(intermediate_output, attention_output)\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        all_hidden = []\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, attention_mask)\n",
    "            all_hidden.append(hidden_states)\n",
    "        return hidden_states, all_hidden\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        cls_token = hidden_states[:, 0]\n",
    "        return self.activation(self.dense(cls_token))\n",
    "\n",
    "class BertModelCustom(nn.Module):\n",
    "    def __init__(self, config: BertConfigCustom):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        if attention_mask is not None:\n",
    "            extended_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            extended_mask = (1.0 - extended_mask) * -10000.0\n",
    "        else:\n",
    "            extended_mask = None\n",
    "\n",
    "        emb = self.embeddings(input_ids, token_type_ids)\n",
    "        seq_out, all_hidden = self.encoder(emb, extended_mask)\n",
    "        pooled = self.pooler(seq_out)\n",
    "        return seq_out, pooled, all_hidden\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset wrapper for SQuAD-style QA data.\n",
    "    Expects data as a list of dicts: {\"context\": ..., \"question\": ..., \"answers\": {...}}\n",
    "    \"\"\"\n",
    "    def __init__(self, data, tokenizer, max_length=384, doc_stride=128):\n",
    "        self.examples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.doc_stride = doc_stride\n",
    "\n",
    "        for entry in data:\n",
    "            inputs = tokenizer(\n",
    "                entry['question'], entry['context'],\n",
    "                truncation=\"only_second\",\n",
    "                max_length=self.max_length,\n",
    "                stride=self.doc_stride,\n",
    "                return_overflowing_tokens=True,\n",
    "                return_offsets_mapping=True,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "            for i, offset in enumerate(inputs['offset_mapping']):\n",
    "                sample = {\n",
    "                    'input_ids': torch.tensor(inputs['input_ids'][i]),\n",
    "                    'attention_mask': torch.tensor(inputs['attention_mask'][i]),\n",
    "                }\n",
    "                answer = entry['answers']['text'][0]\n",
    "                start_char = entry['answers']['answer_start'][0]\n",
    "                end_char = start_char + len(answer)\n",
    "\n",
    "                sequence_ids = inputs.sequence_ids(i)\n",
    "                token_start = 0\n",
    "                while sequence_ids[token_start] != 1:\n",
    "                    token_start += 1\n",
    "                token_end = len(inputs['input_ids'][i]) - 1\n",
    "                while sequence_ids[token_end] != 1:\n",
    "                    token_end -= 1\n",
    "\n",
    "                if not (offset[token_start][0] <= start_char and offset[token_end][1] >= end_char):\n",
    "                    sample['start_positions'] = torch.tensor(0)\n",
    "                    sample['end_positions'] = torch.tensor(0)\n",
    "                else:\n",
    "                    while token_start < len(offset) and offset[token_start][0] <= start_char:\n",
    "                        token_start += 1\n",
    "                    sample['start_positions'] = torch.tensor(token_start - 1)\n",
    "                    while token_end >= 0 and offset[token_end][1] >= end_char:\n",
    "                        token_end -= 1\n",
    "                    sample['end_positions'] = torch.tensor(token_end + 1)\n",
    "\n",
    "                self.examples.append(sample)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "class BertForQuestionAnsweringCustom(nn.Module):\n",
    "    def __init__(self, config: BertConfigCustom):\n",
    "        super().__init__()\n",
    "        self.bert = BertModelCustom(config)\n",
    "        hidden_size = config.hidden_size\n",
    "        self.qa_outputs = nn.Linear(hidden_size, 2)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                start_positions=None,\n",
    "                end_positions=None):\n",
    "        seq_out, _, _ = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        sequence_output = self.dropout(seq_out)\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits   = end_logits.squeeze(-1)\n",
    "\n",
    "        loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss_start = loss_fct(start_logits, start_positions)\n",
    "            loss_end   = loss_fct(end_logits,   end_positions)\n",
    "            loss = (loss_start + loss_end) / 2\n",
    "\n",
    "        return QuestionAnsweringModelOutput(\n",
    "            loss=loss,\n",
    "            start_logits=start_logits,\n",
    "            end_logits=end_logits,\n",
    "            hidden_states=None,\n",
    "            attentions=None\n",
    "        )\n",
    "\n",
    "# ---------- Training & Evaluation Continued ----------\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            start_positions=start_positions,\n",
    "            end_positions=end_positions\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# ---------- Inference Function ----------\n",
    "def evaluate(model, tokenizer, question, context, device):\n",
    "    model.eval()\n",
    "    enc = tokenizer(question, context, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
    "    for k in enc: enc[k] = enc[k].to(device)\n",
    "    outputs = model(enc['input_ids'], enc['attention_mask'], enc.get('token_type_ids', None))\n",
    "    start = torch.argmax(outputs.start_logits, dim=1).item()\n",
    "    end = torch.argmax(outputs.end_logits, dim=1).item() + 1\n",
    "    answer = tokenizer.decode(enc['input_ids'][0][start:end])\n",
    "    return answer\n",
    "\n",
    "def evaluate(question, model, tokenizer, device):\n",
    "    # Compute question embedding\n",
    "    enc = tokenizer(question, return_tensors='pt', truncation=True, padding=True, max_length=256).to(device)\n",
    "    with torch.no_grad():\n",
    "        _, q_emb, _ = model.bert(enc['input_ids'], enc['attention_mask'], enc.get('token_type_ids', None))\n",
    "    q_emb = q_emb.squeeze(0)  # (hidden_size)\n",
    "\n",
    "    # Compute similarities and select best context\n",
    "    sims = F.cosine_similarity(q_emb.unsqueeze(0), CONTEXT_EMB, dim=1)\n",
    "    best_idx = torch.argmax(sims).item()\n",
    "    context = CONTEXTS[best_idx]\n",
    "\n",
    "    # Perform QA on selected context\n",
    "    enc2 = tokenizer(question, context, return_tensors='pt', truncation=True, padding=True, max_length=256).to(device)\n",
    "    outputs = model(enc2['input_ids'], enc2['attention_mask'], enc2.get('token_type_ids', None))\n",
    "    start = torch.argmax(outputs.start_logits, dim=1).item()\n",
    "    end = torch.argmax(outputs.end_logits, dim=1).item() + 1\n",
    "    answer = tokenizer.decode(enc2['input_ids'][0][start:end])\n",
    "    return answer, best_idx\n",
    "\n",
    "def build_context_embeddings(model, tokenizer, contexts, device):\n",
    "    model.eval()\n",
    "    embs = []\n",
    "    for ctx in contexts:\n",
    "        enc = tokenizer(ctx, return_tensors='pt', truncation=True, padding=True, max_length=256).to(device)\n",
    "        with torch.no_grad():\n",
    "            _, pooled, _ = model.bert(enc['input_ids'], enc['attention_mask'], enc.get('token_type_ids', None))\n",
    "        embs.append(pooled.squeeze(0))\n",
    "    return torch.stack(embs)  # (num_ctx, hidden_size)\n",
    "  \n",
    "# ---------- Chain-of-Thought Reasoning Modification ----------\n",
    "class BertForQuestionAnsweringWithReasoning(nn.Module):\n",
    "    def __init__(self, config: BertConfigCustom):\n",
    "        super().__init__()\n",
    "        self.bert = BertModelCustom(config)\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "        self.reasoning_output = nn.Linear(config.hidden_size, config.reasoning_vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n",
    "                start_positions=None, end_positions=None, reasoning_labels=None):\n",
    "        seq_out, hidden_states, attentions = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        logits = self.qa_outputs(seq_out)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        reasoning_logits = self.reasoning_output(seq_out[:, 0])  # [CLS]\n",
    "\n",
    "        loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            loss = (start_loss + end_loss) / 2\n",
    "            if reasoning_labels is not None:\n",
    "                reason_loss = loss_fct(reasoning_logits, reasoning_labels)\n",
    "                loss = loss + reason_loss\n",
    "\n",
    "        return QuestionAnsweringModelOutput(\n",
    "            loss=loss,\n",
    "            start_logits=start_logits,\n",
    "            end_logits=end_logits,\n",
    "            hidden_states=hidden_states,\n",
    "            attentions=attentions,\n",
    "            reasoning_logits=reasoning_logits\n",
    "        )\n",
    "\n",
    "# ---------- Dataset Class with Reasoning Support ----------\n",
    "class QADatasetWithReasoning(Dataset):\n",
    "    def __init__(self, data, tokenizer, reasoning_vocab, max_length=256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.reasoning_vocab = reasoning_vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self): return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        context = item['context']\n",
    "        question = item['question']\n",
    "        answer = item['answers']['text'][0]\n",
    "        start_char = item['answers']['answer_start'][0]\n",
    "        end_char = start_char + len(answer)\n",
    "        reasoning_label = self.reasoning_vocab.get(item.get('reasoning_label', 'factual'), 0)\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            question, context,\n",
    "            truncation='only_second',\n",
    "            max_length=self.max_length,\n",
    "            return_offsets_mapping=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        offsets = enc.pop('offset_mapping')[0]\n",
    "        input_ids = enc['input_ids'].squeeze(0)\n",
    "        attention_mask = enc['attention_mask'].squeeze(0)\n",
    "        token_type_ids = enc.get('token_type_ids', None)\n",
    "        if token_type_ids is not None: token_type_ids = token_type_ids.squeeze(0)\n",
    "\n",
    "        start_pos = end_pos = 0\n",
    "        for i, (s, e) in enumerate(offsets.tolist()):\n",
    "            if s <= start_char < e: start_pos = i\n",
    "            if s < end_char <= e:\n",
    "                end_pos = i\n",
    "                break\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'start_positions': torch.tensor(start_pos),\n",
    "            'end_positions': torch.tensor(end_pos),\n",
    "            'reasoning_labels': torch.tensor(reasoning_label)\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34ca5d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Q&A dataset from a PDF file\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# Load data\n",
    "import fitz\n",
    "import nltk\n",
    "# nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Extract text\n",
    "doc = fitz.open('datasets/alice.pdf')\n",
    "text = ''.join([p.get_text() for p in doc])\n",
    "sentences = sent_tokenize(text)\n",
    "passages = [' '.join(sentences[i:i+5]) for i in range(0, len(sentences), 5)]\n",
    "\n",
    "# Build QA examples\n",
    "qa_examples = []\n",
    "for i, p in enumerate(passages[:]):\n",
    "    question = f\"What is the main idea in passage {i}?\"\n",
    "    answer = sentences[i*5] if i*5 < len(sentences) else ''\n",
    "    start_idx = p.find(answer)\n",
    "    qa_examples.append({\n",
    "        'context': p,\n",
    "        'question': question,\n",
    "        'answers': {'text': [answer], 'answer_start': [start_idx]},\n",
    "        'reasoning_label': 'factual'\n",
    "    })\n",
    "\n",
    "# Save\n",
    "with open('datasets/alice_qa.json', 'w') as f:\n",
    "    json.dump(qa_examples, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a0da61ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devices: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load Q&A and prepare\n",
    "with open('datasets/alice_qa.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "vocab = {'factual': 0, 'causal': 1, 'comparative': 2}\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "dataset = QADatasetWithReasoning(data, tokenizer, vocab)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Model setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config = BertConfigCustom(reasoning_vocab_size=len(vocab))\n",
    "model = BertForQuestionAnsweringWithReasoning(config).to(device)\n",
    "opt = AdamW(model.parameters(), lr=3e-5)\n",
    "CONTEXTS = [ex['context'] for ex in data]\n",
    "CONTEXT_EMB = build_context_embeddings(model, tokenizer, CONTEXTS, device)\n",
    "\n",
    "print(\"devices:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd67c400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.7508\n",
      "Epoch 2 Loss: 1.2528\n",
      "Epoch 3 Loss: 1.1195\n",
      "Epoch 4 Loss: 1.0654\n",
      "Epoch 5 Loss: 0.9838\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m     out\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     16\u001b[0m     opt\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 17\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(loader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        opt.zero_grad()\n",
    "        out = model(\n",
    "            batch['input_ids'].to(device),\n",
    "            batch['attention_mask'].to(device),\n",
    "            batch.get('token_type_ids', None),\n",
    "            batch['start_positions'].to(device),\n",
    "            batch['end_positions'].to(device),\n",
    "            batch['reasoning_labels'].to(device)\n",
    "        )\n",
    "        out.loss.backward()\n",
    "        opt.step()\n",
    "        total_loss += out.loss.item()\n",
    "    print(f'Epoch {epoch+1} Loss: {total_loss/len(loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e81bf8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "os.makedirs(\"models/qa_model\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"models/my_gpt2_rag_model.pth\") # model.state_dict()\n",
    "tokenizer.save_pretrained(\"models/qa_model\")\n",
    "print(\"Model and tokenizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c16bc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnsweringWithReasoning(\n",
       "  (bert): BertModelCustom(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELU(approximate='none')\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (reasoning_output): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model for evaluation\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(\"models/qa_model\")\n",
    "model = BertForQuestionAnsweringWithReasoning(config)\n",
    "model.load_state_dict(torch.load(\"models/my_gpt2_rag_model.pth\"))\n",
    "model.to(device)\n",
    "# model.load_state_dict(torch.load(\"models/my_gpt3_model.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0e51fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the main idea in passage 4?\n",
      "A: in another moment down went alice after it, never once considering how in the world she was to get out again.\n",
      "E: In another moment down went Alice after it, never once considering how\n",
      "in the world she was to get out again.\n",
      " --------------------------------------------------\n",
      "Q: What is the main idea in passage 90?\n",
      "A: “ come, it ’ s pleased so far, ” thought alice, and she went on.\n",
      "E: “Come, it’s pleased so far,” thought Alice, and she went on.\n",
      " --------------------------------------------------\n",
      "Q: What is the main idea in passage 56?\n",
      "A: heads below! ” ( a loud crash ) — “ now, who did that? — it was bill, i fancy — who ’ s to go down the chimney? — nay, _ i _ shan ’ t!\n",
      "E: Heads below!” (a loud crash)—“Now, who did that?—It was Bill, I\n",
      "fancy—Who’s to go down the chimney?—Nay, _I_ shan’t!\n",
      " --------------------------------------------------\n",
      "Q: What is the main idea in passage 18?\n",
      "A: “ you ought to be ashamed of yourself, ” said alice, “ a great girl like you, ” ( she might well say this ), “ to go on crying in this way! stop this moment, i tell you! ” but she went on all the same, shedding gallons of tears, until there was a large pool all round her, about four inches deep and reaching half down the hall.\n",
      "E: “You ought to be ashamed of yourself,” said Alice, “a great girl like\n",
      "you,” (she might well say this), “to go on crying in this way!\n",
      " --------------------------------------------------\n",
      "Q: What is the main idea in passage 94?\n",
      "A: “ by - the - bye, what became of the baby? ” said the cat.\n",
      "E: “By-the-bye, what became of the baby?” said the Cat.\n",
      " --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# evaluate the reasoning model\n",
    "import random\n",
    "for i in range(5):\n",
    "    idx = random.randint(0, len(data) - 1)\n",
    "    q = data[idx]['question']\n",
    "    c = data[idx]['context']\n",
    "    expected = data[idx]['answers']['text'][0]\n",
    "    ans = evaluate(model, tokenizer, q, c, device)\n",
    "    print(f\"Q: {q}\\nA: {ans}\\nE: {expected}\\n {'-'*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba126851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Who is Lewis Carroll?\n",
      "A: is lewis carroll? [SEP] “ beautiful soup! who cares for fish, game, or any other dish? who would not give all else for two p enny\n",
      "E: Author of the book alice's adventures in wonderland\n",
      "C: “Beautiful Soup! Who cares for fish,\n",
      "Game, or any other dish? Who would not give all else for two p\n",
      "ennyworth only of beautiful Soup? Pennyworth only of beautiful Soup? Beau—ootiful Soo—oop!\n",
      " --------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "question = \"Who is Lewis Carroll?\"\n",
    "ans, idx = evaluate(question, model, tokenizer, device)\n",
    "expected = \"Author of the book alice's adventures in wonderland\"\n",
    "print(f\"Q: {question}\\nA: {ans}\\nE: {expected}\\nC: {CONTEXTS[idx]}\\n {'-'*50}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
