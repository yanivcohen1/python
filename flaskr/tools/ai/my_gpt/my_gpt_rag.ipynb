{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc135a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nnfrom torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "# BERT stands for Bidirectional Encoder Representations from Transformers.\n",
    "# It is a transformer-based model designed to understand the context of words in a sentence.\n",
    "# It was introduced by Google in 2018 and has since become a foundational model for many NLP tasks.\n",
    "\n",
    "# ---------- Custom Output Class ----------\n",
    "class QuestionAnsweringModelOutput:\n",
    "    def __init__(self, loss=None, start_logits=None, end_logits=None, hidden_states=None, attentions=None, reasoning_logits=None):\n",
    "        self.reasoning_logits = reasoning_logits\n",
    "        self.loss = loss\n",
    "        self.start_logits = start_logits\n",
    "        self.end_logits = end_logits\n",
    "        self.hidden_states = hidden_states\n",
    "        self.attentions = attentions\n",
    "\n",
    "# ---------- Custom BERT Implementation ----------\n",
    "class BertConfigCustom:\n",
    "    def __init__(self,\n",
    "                 vocab_size=30522,\n",
    "                 hidden_size=768,\n",
    "                 num_hidden_layers=12,\n",
    "                 num_attention_heads=12,\n",
    "                 intermediate_size=3072,\n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 attention_probs_dropout_prob=0.1,\n",
    "                 max_position_embeddings=512,\n",
    "                 type_vocab_size=2,\n",
    "                 reasoning_vocab_size=3):\n",
    "        self.reasoning_vocab_size = reasoning_vocab_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "\n",
    "class BertEmbeddings(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        words = self.word_embeddings(input_ids)\n",
    "        positions = self.position_embeddings(position_ids)\n",
    "        types = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = words + positions + types\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        return self.dropout(embeddings)\n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\"Hidden size must be divisible by number of heads\")\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = config.hidden_size // self.num_heads\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.key   = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.value = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "        def transpose(x):\n",
    "            return x.view(batch_size, seq_len, self.num_heads, self.head_dim) \\\n",
    "                    .permute(0, 2, 1, 3)\n",
    "\n",
    "        Q = transpose(self.query(hidden_states))\n",
    "        K = transpose(self.key(hidden_states))\n",
    "        V = transpose(self.value(hidden_states))\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2))\n",
    "        scores = scores / math.sqrt(self.head_dim)\n",
    "        if attention_mask is not None:\n",
    "            scores = scores + attention_mask\n",
    "        probs = torch.softmax(scores, dim=-1)\n",
    "        probs = self.dropout(probs)\n",
    "\n",
    "        context = torch.matmul(probs, V)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous()\n",
    "        return context.view(batch_size, seq_len, -1)\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden = self.dense(hidden_states)\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.LayerNorm(hidden + input_tensor)\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        self_output = self.self(hidden_states, attention_mask)\n",
    "        return self.output(self_output, hidden_states)\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.intermediate_act_fn = nn.GELU()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        return self.intermediate_act_fn(self.dense(hidden_states))\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden = self.dense(hidden_states)\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.LayerNorm(hidden + input_tensor)\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = BertAttention(config)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        return self.output(intermediate_output, attention_output)\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None):\n",
    "        all_hidden = []\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, attention_mask)\n",
    "            all_hidden.append(hidden_states)\n",
    "        return hidden_states, all_hidden\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        cls_token = hidden_states[:, 0]\n",
    "        return self.activation(self.dense(cls_token))\n",
    "\n",
    "class BertModelCustom(nn.Module):\n",
    "    def __init__(self, config: BertConfigCustom):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        self.encoder = BertEncoder(config)\n",
    "        self.pooler = BertPooler(config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        if attention_mask is not None:\n",
    "            extended_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "            extended_mask = (1.0 - extended_mask) * -10000.0\n",
    "        else:\n",
    "            extended_mask = None\n",
    "\n",
    "        emb = self.embeddings(input_ids, token_type_ids)\n",
    "        seq_out, all_hidden = self.encoder(emb, extended_mask)\n",
    "        pooled = self.pooler(seq_out)\n",
    "        return seq_out, pooled, all_hidden\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset wrapper for SQuAD-style QA data.\n",
    "    Expects data as a list of dicts: {\"context\": ..., \"question\": ..., \"answers\": {...}}\n",
    "    \"\"\"\n",
    "    def __init__(self, data, tokenizer, max_length=384, doc_stride=128):\n",
    "        self.examples = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.doc_stride = doc_stride\n",
    "\n",
    "        for entry in data:\n",
    "            inputs = tokenizer(\n",
    "                entry['question'], entry['context'],\n",
    "                truncation=\"only_second\",\n",
    "                max_length=self.max_length,\n",
    "                stride=self.doc_stride,\n",
    "                return_overflowing_tokens=True,\n",
    "                return_offsets_mapping=True,\n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "            for i, offset in enumerate(inputs['offset_mapping']):\n",
    "                sample = {\n",
    "                    'input_ids': torch.tensor(inputs['input_ids'][i]),\n",
    "                    'attention_mask': torch.tensor(inputs['attention_mask'][i]),\n",
    "                }\n",
    "                answer = entry['answers']['text'][0]\n",
    "                start_char = entry['answers']['answer_start'][0]\n",
    "                end_char = start_char + len(answer)\n",
    "\n",
    "                sequence_ids = inputs.sequence_ids(i)\n",
    "                token_start = 0\n",
    "                while sequence_ids[token_start] != 1:\n",
    "                    token_start += 1\n",
    "                token_end = len(inputs['input_ids'][i]) - 1\n",
    "                while sequence_ids[token_end] != 1:\n",
    "                    token_end -= 1\n",
    "\n",
    "                if not (offset[token_start][0] <= start_char and offset[token_end][1] >= end_char):\n",
    "                    sample['start_positions'] = torch.tensor(0)\n",
    "                    sample['end_positions'] = torch.tensor(0)\n",
    "                else:\n",
    "                    while token_start < len(offset) and offset[token_start][0] <= start_char:\n",
    "                        token_start += 1\n",
    "                    sample['start_positions'] = torch.tensor(token_start - 1)\n",
    "                    while token_end >= 0 and offset[token_end][1] >= end_char:\n",
    "                        token_end -= 1\n",
    "                    sample['end_positions'] = torch.tensor(token_end + 1)\n",
    "\n",
    "                self.examples.append(sample)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "class BertForQuestionAnsweringCustom(nn.Module):\n",
    "    def __init__(self, config: BertConfigCustom):\n",
    "        super().__init__()\n",
    "        self.bert = BertModelCustom(config)\n",
    "        hidden_size = config.hidden_size\n",
    "        self.qa_outputs = nn.Linear(hidden_size, 2)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                start_positions=None,\n",
    "                end_positions=None):\n",
    "        seq_out, _, _ = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        sequence_output = self.dropout(seq_out)\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits   = end_logits.squeeze(-1)\n",
    "\n",
    "        loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss_start = loss_fct(start_logits, start_positions)\n",
    "            loss_end   = loss_fct(end_logits,   end_positions)\n",
    "            loss = (loss_start + loss_end) / 2\n",
    "\n",
    "        return QuestionAnsweringModelOutput(\n",
    "            loss=loss,\n",
    "            start_logits=start_logits,\n",
    "            end_logits=end_logits,\n",
    "            hidden_states=None,\n",
    "            attentions=None\n",
    "        )\n",
    "\n",
    "# ---------- Training & Evaluation Continued ----------\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            start_positions=start_positions,\n",
    "            end_positions=end_positions\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, tokenizer, question, context, device):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=384\n",
    "    )\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    token_type_ids = inputs.get('token_type_ids', None)\n",
    "    if token_type_ids is not None:\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "        start_logits = outputs.start_logits\n",
    "        end_logits = outputs.end_logits\n",
    "        start = torch.argmax(start_logits, dim=1).item()\n",
    "        end = torch.argmax(end_logits, dim=1).item() + 1\n",
    "        answer = tokenizer.decode(input_ids[0][start:end])\n",
    "    return answer\n",
    "\n",
    "# ---------- Chain-of-Thought Reasoning Modification ----------\n",
    "class BertForQuestionAnsweringWithReasoning(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(BertForQuestionAnsweringWithReasoning, self).__init__()\n",
    "        self.bert = BertModelCustom(config)  # Replaced with custom model\n",
    "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
    "        self.reasoning_output = nn.Linear(config.hidden_size, config.reasoning_vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None,\n",
    "                start_positions=None, end_positions=None, reasoning_labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        sequence_output = outputs[0]\n",
    "        hidden_states = outputs[1] if len(outputs) > 1 else None\n",
    "        attentions = outputs[2] if len(outputs) > 2 else None\n",
    "\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        # Reasoning prediction (optional label)\n",
    "        reasoning_logits = self.reasoning_output(sequence_output[:, 0])  # Use [CLS] token\n",
    "\n",
    "        total_loss = None\n",
    "        if start_positions is not None and end_positions is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            start_loss = loss_fct(start_logits, start_positions)\n",
    "            end_loss = loss_fct(end_logits, end_positions)\n",
    "            total_loss = (start_loss + end_loss) / 2\n",
    "\n",
    "            if reasoning_labels is not None:\n",
    "                reasoning_loss = loss_fct(reasoning_logits, reasoning_labels)\n",
    "                total_loss += reasoning_loss\n",
    "\n",
    "        return QuestionAnsweringModelOutput(\n",
    "            loss=total_loss,\n",
    "            start_logits=start_logits,\n",
    "            end_logits=end_logits,\n",
    "            hidden_states=hidden_states,\n",
    "            attentions=attentions,\n",
    "            reasoning_logits=reasoning_logits\n",
    "        )\n",
    "\n",
    "# ---------- Dataset Class with Reasoning Support ----------\n",
    "# from torch.utils.data import Dataset\n",
    "class QADatasetWithReasoning(Dataset):\n",
    "    def __init__(self, data, tokenizer, reasoning_vocab, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.reasoning_vocab = reasoning_vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        context = item['context']\n",
    "        question = item['question']\n",
    "        answer = item['answers']['text'][0]\n",
    "        start_char = item['answers']['answer_start'][0]\n",
    "        end_char = start_char + len(answer)\n",
    "        reasoning_label = self.reasoning_vocab.get(item.get('reasoning_label', 'factual'), 0)\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            question,\n",
    "            context,\n",
    "            truncation=\"only_second\",\n",
    "            max_length=self.max_length,\n",
    "            stride=128,\n",
    "            return_overflowing_tokens=False,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        offset_mapping = encoding.pop(\"offset_mapping\")[0]\n",
    "        input_ids = encoding[\"input_ids\"][0]\n",
    "        start_positions = end_positions = 0\n",
    "\n",
    "        for i, (start, end) in enumerate(offset_mapping):\n",
    "            if start <= start_char < end:\n",
    "                start_positions = i\n",
    "            if start < end_char <= end:\n",
    "                end_positions = i\n",
    "                break\n",
    "\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        encoding[\"start_positions\"] = torch.tensor(start_positions)\n",
    "        encoding[\"end_positions\"] = torch.tensor(end_positions)\n",
    "        encoding[\"reasoning_labels\"] = torch.tensor(reasoning_label)\n",
    "\n",
    "        return encoding\n",
    "  \n",
    "# loading necessary libraries\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Tokenizer and Config\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "config = BertConfigCustom()\n",
    "\n",
    "# Model and Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForQuestionAnsweringCustom(config).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "print(\"device:\", device)\n",
    "\n",
    "# Reasoning types example:\n",
    "# \"reasoning_vocab_size\": 3\n",
    "# 0 = factual, 1 = causal, 2 = comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cd67c400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at epoch 1 with loss 0.0228\n",
      "Epoch 1: loss = 0.0228\n",
      "Epoch 2: loss = 0.0379\n",
      "Epoch 3: loss = 0.0384\n",
      "Model saved at epoch 4 with loss 0.0120\n",
      "Epoch 4: loss = 0.0120\n",
      "Epoch 5: loss = 0.0239\n",
      "Epoch 6: loss = 0.0165\n",
      "Epoch 7: loss = 0.0264\n",
      "Epoch 8: loss = 0.0418\n",
      "Epoch 9: loss = 0.0257\n",
      "Epoch 10: loss = 0.0544\n",
      "Epoch 11: loss = 0.0341\n",
      "Epoch 12: loss = 0.0213\n",
      "Epoch 13: loss = 0.0373\n",
      "Epoch 14: loss = 0.0270\n",
      "Epoch 15: loss = 0.0464\n",
      "Epoch 16: loss = 0.0232\n",
      "Epoch 17: loss = 0.0305\n",
      "Epoch 18: loss = 0.0231\n",
      "Epoch 19: loss = 0.0143\n",
      "Epoch 20: loss = 0.0304\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "# current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# Load data\n",
    "with open(\"datasets/train_resening_data.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = QADataset(data, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# start training\n",
    "min_loss = float('inf')\n",
    "best_model = None\n",
    "for epoch in range(20):\n",
    "    loss = train(model, dataloader, optimizer, device)\n",
    "    if min_loss > loss:\n",
    "        min_loss = loss\n",
    "        best_model = model.state_dict()\n",
    "        # torch.save(model.state_dict(), \"models/gpt_resening_model.pth\")\n",
    "        print(f\"Model saved at epoch {epoch+1} with loss {loss:.4f}\")\n",
    "    print(f\"Epoch {epoch+1}: loss = {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e81bf8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "os.makedirs(\"models/qa_model\", exist_ok=True)\n",
    "if best_model:\n",
    "    torch.save(best_model, \"models/gpt_resening_model.pth\") # model.state_dict()\n",
    "    tokenizer.save_pretrained(\"models/qa_model\")\n",
    "    print(\"Model and tokenizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9c16bc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForQuestionAnsweringCustom(\n",
       "  (bert): BertModelCustom(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELU(approximate='none')\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model for evaluation\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(\"models/qa_model\")\n",
    "model = BertForQuestionAnsweringCustom(config)\n",
    "model.load_state_dict(torch.load(\"models/gpt_resening_model.pth\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e0e51fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Where is the Eiffel Tower?\n",
      "A: paris. it\n"
     ]
    }
   ],
   "source": [
    "# evaluate the reasoning model\n",
    "question = \"Where is the Eiffel Tower?\"\n",
    "context = \"The Eiffel Tower is in Paris. It is one of the most visited landmarks in the world.\"\n",
    "answer = evaluate(model, tokenizer, question, context, device)\n",
    "print(f\"Q: {question}\\nA: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7f2d53b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Where is the most visited landmarks in the world?\n",
      "A: \n"
     ]
    }
   ],
   "source": [
    "# evaluate the reasoning model\n",
    "question = \"Where is the most visited landmarks in the world?\"\n",
    "context = \"The Eiffel Tower is in Paris. It is one of the most visited landmarks in the world.\"\n",
    "answer = evaluate(model, tokenizer, question, context, device)\n",
    "print(f\"Q: {question}\\nA: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7353c2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: what Guido van Rossum created?\n",
      "A: \n"
     ]
    }
   ],
   "source": [
    "# evaluate the reasoning model\n",
    "question = \"what Guido van Rossum created?\"\n",
    "context = \"Python is a programming language that emphasizes code readability. It was created by Guido van Rossum.\"\n",
    "answer = evaluate(model, tokenizer, question, context, device)\n",
    "print(f\"Q: {question}\\nA: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
